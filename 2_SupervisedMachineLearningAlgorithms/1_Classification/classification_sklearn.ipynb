{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Training a SGD classifier on MNIST Dataset (One class classifcation)* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:\\\\Users\\\\\\Ayush\\\\Desktop\\\\ML PROJECTS\\\\digit-recognizer\\\\data\\\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data.iloc[:,1:], data.iloc[:,0],test_size = 0.2, random_state = 0, shuffle=True)                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33600, 784)\n",
      "(8400, 784)\n",
      "(33600,)\n",
      "(8400,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will train a classifier which will predict 0 vs non-zero digits\n",
    "X_train = train_data.copy()\n",
    "y_train = train_labels.copy()\n",
    "y_train = y_train==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Training the SGD classifer on above data assuming there is no smapling bias and no preprocessing required*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(random_state=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [ True False False False False False False False False False  True]\n",
      "Actual Label: [True, False, False, False, False, False, False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "# predicting on the training set\n",
    "some_data = X_train[100:111][:]\n",
    "some_data_predictions = clf.predict(some_data)\n",
    "some_data_labels = y_train[100:111]\n",
    "print(\"Predictions:\",some_data_predictions)\n",
    "print(\"Actual Label:\",list(some_data_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like SGDClassifier is doing quite well, but we require different metrics to evaluate classification models as it is a bit trickier to evaluate classification metrics as compared to regression metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Different Evaluation Metrics*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Implementing Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98767857, 0.98732143, 0.98642857])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation metrics in cross_val_score  is accuracy\n",
    "from sklearn.model_selection import cross_val_score\n",
    "y_train_score = cross_val_score(clf, X_train, y_train, cv=3)\n",
    "y_train_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. *Confusion Matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_pred.shape =  (33600,)\n",
      "y_train_pred= [False False False ... False  True False]\n",
      "Confusion_matrix= [[30055   226]\n",
      " [  206  3113]]\n",
      "Accuracy Metric =  0.9871428571428572\n",
      "Precision =  0.937933112383248\n",
      "Recall_score =  0.9323150643905361\n"
     ]
    }
   ],
   "source": [
    "# it just returns the predictions on the test_folds\n",
    "from sklearn.model_selection import cross_val_predict \n",
    "y_train_pred = cross_val_predict(clf, X_train, y_train, cv=3)\n",
    "print(\"y_train_pred.shape = \", y_train_pred.shape)\n",
    "print(\"y_train_pred=\",y_train_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion_matrix=\",confusion_matrix(y_train, y_train_pred))\n",
    "'''\n",
    "Each row in a confusion matrix represents an actual class, while each column repre‐\n",
    "sents a predicted class. The first row of this matrix considers non-0 images (the nega‐\n",
    "tive class): 30055 of them were correctly classified as non-0s (they are called true\n",
    "negatives), while the remaining 226 were wrongly classified as 0s (false positives).\n",
    "The second row considers the images of 0s (the positive class): 206 were wrongly\n",
    "classified as non-0s (false negatives), while the remaining 3113 were correctly classi‐\n",
    "fied as 5s (true positives). \n",
    "'''\n",
    "\n",
    " '''\n",
    " A prefect confusion matrix would have zeroes in its off diagonal and tr(cm) = num_examples\n",
    " Sometimes we may prefer concise metric (kam shabdo main jyada baat), one of them for classification is\n",
    " precision of the classifer; precision = TP/(TP + FP), sometimes this precision matrix alone is not enough \n",
    " so, we call it with recall metric; recall = TP/(TP + FN)\n",
    " we can even combine the two metrics as F1 score which is basically the harmonic mean of precision and \n",
    " recall metrics'''\n",
    "\n",
    "'''\n",
    "The F1 score favors classifiers that have similar precision and recall. This is not always\n",
    "what you want: in some contexts you mostly care about precision, and in other con‐\n",
    "texts you really care about recall. For example, if you trained a classifier to detect vid‐\n",
    "eos that are safe for kids, you would probably prefer a classifier that rejects many\n",
    "good videos (low recall) but keeps only safe ones (high precision), rather than a clas‐\n",
    "sifier that has a much higher recall but lets a few really bad videos show up in your\n",
    "product (in such cases, you may even want to add a human pipeline to check the clas‐\n",
    "sifier’s video selection). On the other hand, suppose you train a classifier to detect\n",
    "shoplifters on surveillance images: it is probably fine if your classifier has only 30%\n",
    "precision as long as it has 99% recall (sure, the security guards will get a few false\n",
    "alerts, but almost all shoplifters will get caught).\n",
    "Unfortunately, you can’t have it both ways: increasing precision reduces recall, and\n",
    "vice versa. This is called the precision/recall tradeof.\n",
    "'''\n",
    "print(\"Accuracy Metric = \",y_train_score.mean())\n",
    "print(\"Precision = \",3113/(3113 + 226))\n",
    "print(\"Recall_score = \", 3113/(3113 + 206))\n",
    "# clearly, our accuracy metric isn't that great way to measure accuracy in classification tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
