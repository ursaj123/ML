{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating random samples for linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regression_samples(num_examples, num_features):\n",
    "    orig_weight = 5*np.random.randn(num_features)\n",
    "    orig_bias = np.random.randn(1)\n",
    "    \n",
    "    x = 5*np.random.randn(num_examples, num_features)\n",
    "    y = np.array([x[i]@orig_weight + orig_bias + np.random.randint(1, 5, 1)*np.random.randn(1) for i in range(num_examples)])\n",
    "    y/=10\n",
    "    return x, y, orig_weight, orig_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Stochastic gradient descent update are - \n",
    "    \n",
    "    $w$ = $w$ - $(\\eta/\\beta)$*$\\sum_{i\\in\\beta}x^{(i)}(w^Tx + b- y^{(i)})$ \n",
    "\n",
    "    $b$ = $b$ - $(\\eta/\\beta)$*$\\sum_{i\\in\\beta}(w^Tx + b- y^{(i)})$ \n",
    "\n",
    "    where $\\eta$ is the learning rate and $\\beta$ is  the batch size.\n",
    "\n",
    "2. Parameters that are tunable but not updated in the training loop are called hyperparameters. Hyperparameter tuning is the process by which hyperparameters are chosen, and typically requires that we adjust them basedon the results of the training loop as assessed on a separate validation dataset (or validation set).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch generator, mainly used so that we can take benefit of GPU's, and it is memory efficient as well \n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = labels.shape[0]\n",
    "    indices = list(range(num_examples))\n",
    "    # randomly shuffling the examples first\n",
    "    np.random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = indices[i:min(i+batch_size, num_examples)]\n",
    "        yield features[batch_indices], labels[batch_indices] \n",
    "        ## more about yield  - https://www.geeksforgeeks.org/python-yield-keyword/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
