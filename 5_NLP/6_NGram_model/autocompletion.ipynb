{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baf26905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from time import time \n",
    "import math as m\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# will come into use for nltk word_tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f3f224",
   "metadata": {},
   "source": [
    "### ***TEXT PREPOCESSSING***\n",
    "- Figuring out all the messages by me only.\n",
    "- Remove all the emojis.\n",
    "- Remove all the links.\n",
    "- Convert all strings to lower.\n",
    "- Remove all the numbers.\n",
    "- Tokenize the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78152b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack overflow zindabaad\n",
    "# to remove all the emojis from a string \n",
    "def deEmojify(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f9a716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(string):\n",
    "    string = string.lower()\n",
    "    # i have to remove links in the message if any\n",
    "    string = re.sub(r'https\\S+', '', string)  # it will remove all of the link as we know there is no whitespace in the link\n",
    "    # i have to remove emojis if any \n",
    "    string = deEmojify(string)\n",
    "    # i have to remove numerics if any\n",
    "    string = re.sub(r'[0-9]', '', string)\n",
    "    # to remove spaces at the start and end of the sentences\n",
    "    string = string.strip() \n",
    "    # now we must toeknize sentence\n",
    "    return nltk.word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb6d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy code\n",
    "def PREPROCESSED_DATA(data):\n",
    "    data = data.split('\\n')\n",
    "    #data = [data]  # wapas hatana hai\n",
    "    # keval mere likhne ka pattern hi check karna hai mujhe isliye main keval meri chats hi utha raha hoon, although chats realted\n",
    "    # hain but main maan kar chal raha hoon ki chats independent hain (matlab pichle message par depend karta hai ki aage kya likhne waala hoon)    \n",
    "    required_messages = []\n",
    "    for s in data:\n",
    "        idx = s.find('-')\n",
    "        temp = s[idx+1:]\n",
    "        #print(f\"s = {s}\\ntemp = {temp}\")\n",
    "        idx = temp.find(':')\n",
    "        #print(f\"temp = {temp}\\n\\n\\n\")\n",
    "        if idx!=-1 and not temp.endswith('>'):  # considering my messages only and ignoring the messages in which media were sent\n",
    "            temp = temp[idx+1:]\n",
    "            required_messages.append(temp)  # +14 isliye taaki naam aur ': ' dono avoid ho jaaye aur keval sentence hi mile\n",
    "        # now that data is ready i have to preprocess it\n",
    "    \n",
    "    preprocessed_data = []\n",
    "    # now we have to preprocess each message\n",
    "    for message in required_messages:\n",
    "        temp = preprocessing(message)\n",
    "        if len(temp)>0:\n",
    "            preprocessed_data.append(temp)\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b87604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isme keval khud ki chats hi daali hain (as told by binod)\n",
    "def PREPROCESSED_DATA(data):\n",
    "    data = data.split('\\n')\n",
    "    # keval mere likhne ka pattern hi check karna hai mujhe isliye main keval meri chats hi utha raha hoon, although chats realted\n",
    "    # hain but main maan kar chal raha hoon ki chats independent hain (matlab pichle message par depend karta hai ki aage kya likhne waala hoon)    \n",
    "    required_messages = []\n",
    "    for s in data:\n",
    "        temp = s.find('Binod')\n",
    "        if temp!=-1 and not s.endswith('>'):  # considering my messages only and ignoring the messages in which media were sent\n",
    "            required_messages.append(s[temp + 7:])  # +14 isliye taaki naam aur ': ' dono avoid ho jaaye aur keval sentence hi mile\n",
    "        # now that data is ready i have to preprocess it\n",
    "    \n",
    "    preprocessed_data = []\n",
    "    # now we have to preprocess each message\n",
    "    for message in required_messages:\n",
    "        temp = preprocessing(message)\n",
    "        if len(temp)>0:\n",
    "            preprocessed_data.append(temp)\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da666b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"WhatsApp Chat with IIT BHU Mnc 3rd year.txt\"\n",
    "# now collecting the data and will try if it work with my way of talking \n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "preprocessed_data = PREPROCESSED_DATA(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce1c3282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: ['this', 'message', 'was', 'deleted']\n",
      "2: ['st', 'msg']\n",
      "3: ['welcome', 'gumys']\n",
      "4: ['we', 'are', 'here', 'the', 'raise', 'the', 'voice', 'against', 'the', 'repression']\n",
      "5: ['welcome', 'to', 'mnc']\n",
      "6: ['++']\n",
      "7: ['this', 'message', 'was', 'deleted']\n",
      "8: ['++']\n",
      "9: ['++']\n",
      "10: ['this', 'message', 'was', 'deleted']\n",
      "11: ['this', 'message', 'was', 'deleted']\n",
      "12: ['this', 'message', 'was', 'deleted']\n",
      "13: ['this', 'message', 'was', 'deleted']\n",
      "14: ['this', 'message', 'was', 'deleted']\n",
      "15: ['this', 'message', 'was', 'deleted']\n",
      "16: ['smashing', 'nepotism']\n",
      "17: ['smashing', 'racism']\n",
      "18: ['smashing', 'misogyny']\n",
      "19: ['smashing', 'autocracy']\n",
      "20: ['smashing', 'masochism']\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(f'{i+1}: {preprocessed_data[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab40f7",
   "metadata": {},
   "source": [
    "### ***Splitting the dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "904dc915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that preprocessing is done, we have to split the text into training and test sets assuming that my text in independent of the order  \n",
    "def splits(preprocesed_data, train_size=0.95):\n",
    "    np.random.seed(0)  # taaki har baar ek hi split aaye\n",
    "    np.random.shuffle(preprocesed_data)\n",
    "    index = int(len(preprocesed_data)*train_size)\n",
    "    train_data = preprocesed_data[:index]\n",
    "    test_data = preprocesed_data[index:]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b7c51a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15001 0\n"
     ]
    }
   ],
   "source": [
    "train_size = 1.0 # data kam hai abhi\n",
    "train_data, test_data = splits(preprocessed_data, train_size=train_size)\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af9e6d",
   "metadata": {},
   "source": [
    "### ***Vocab Building***\n",
    "- We know that there can be many words in the testing phase which are not in vocabulary and we have to make our model robust of it, so we will add a minimum frequencey barrier in the vocabulary built by training data through which only words having higher frequency will be selected.\n",
    "- Then we will replace words in preprocessed_data by '<unk>' which are not present in the closed vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "169ea7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that splits are done, we have to first form the vocabulary of words\n",
    "# then set a frequencey of words under which no words will be in vocabulary\n",
    "# replace our preprocessed_sets with <unk>\n",
    "def build_vocab(preprocessed_data, min_freq=2):\n",
    "    vocab = {}\n",
    "    closed_vocab = {}\n",
    "    for message in preprocessed_data:\n",
    "        for word in message:\n",
    "            if word in vocab.keys():\n",
    "                vocab[word]+=1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    # now we have to filter out the words in the vocabulary\n",
    "    for word, count in vocab.items():\n",
    "        if count>min_freq:\n",
    "            closed_vocab[word] = count\n",
    "    \n",
    "    return vocab, closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56044068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7181, 3453)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_freq = 1 \n",
    "vocab, closed_vocab = build_vocab(train_data, min_freq=min_freq)\n",
    "# this is a not a good dataset as as almost half of the words are just appearing for 1 time just because of different forms\n",
    "#(singular or plural, gender (acccha ya achchi))\n",
    "len(vocab), len(closed_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81cf74ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_preprocessing(train_data, closed_vocab, unknown_token = '<unk>'):\n",
    "    # have to do preprocessing again, replace many of the words by <unk>\n",
    "    train_data_unk = []\n",
    "    # first for training data\n",
    "    for message in train_data:\n",
    "        temp = []\n",
    "        for word in message:\n",
    "            if word in closed_vocab.keys():\n",
    "                temp.append(word)\n",
    "            else:\n",
    "                temp.append(unknown_token)\n",
    "        train_data_unk.append(temp)\n",
    "    \n",
    "    return train_data_unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00cbee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearly need much more data (itne saare unks hain kisi kisi statement main), par experiment ke liye abhi yahi dataset se\n",
    "# aage badh raha hoon\n",
    "train_data_unk = unk_preprocessing(train_data, closed_vocab)\n",
    "test_data_unk = unk_preprocessing(test_data, closed_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d698a",
   "metadata": {},
   "source": [
    "### ***N-grams***\n",
    " - Here, we will have the markov assumption that the next word of the model will depend upon last n words words.\n",
    " - We will implement the sliding windows and build the n_gram_vocab where the key will be a tuple and its value will be number of counts of that phrase.\n",
    " - Then we have to implement a function for probability of a word occuring given the last n_gram (here we will be implementing a bigram model, i.e., the upcoming word will depend on the last two words).\n",
    " - We have to also consider k-smoothing so that, there is atleast some probability of every word occuring next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c274ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram(train_data_unk, window_len=2):\n",
    "    counts = {}\n",
    "    for message in train_data_unk:\n",
    "        temp =  ['<s>']*(window_len-1)  + message + ['<e>']\n",
    "        for i in range(0, len(temp)-window_len+1):\n",
    "            phrase = tuple(temp[i:i+window_len]) # as list is unhashable type\n",
    "            if phrase in counts.keys():\n",
    "                counts[phrase]+=1\n",
    "            else:\n",
    "                counts[phrase] = 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f20ab38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3455, 29878\n"
     ]
    }
   ],
   "source": [
    "n = 2  # we are working with biagrams, i.e., the next word will be predicted just on the just the previous word only \n",
    "n_gram_counts = n_gram(train_data_unk, window_len=n-1)\n",
    "n_plus_one_gram_counts = n_gram(train_data_unk, window_len=n)\n",
    "print(f\"{len(n_gram_counts)}, {len(n_plus_one_gram_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fded6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prob(previous_n_gram, word, n_gram_counts=n_gram_counts, \n",
    "               n_plus_one_gram_counts = n_plus_one_gram_counts, k = 1.0, vocab_size=len(closed_vocab)): \n",
    "    prev = tuple(previous_n_gram)\n",
    "    c1 = n_gram_counts[prev] if prev in n_gram_counts.keys() else 0\n",
    "    prev = prev + (word, )\n",
    "    c2 = n_plus_one_gram_counts[prev] if prev in n_plus_one_gram_counts.keys() else 0\n",
    "    prob = (c2 + k)/(c1 + k*vocab_size)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7aac0b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of phrase a occuring is 0.000850\n",
      "Probability of phrase b occuring is 0.000283\n",
      "Probability of phrase c occuring os 0.000283\n"
     ]
    }
   ],
   "source": [
    "# \"mujhe pata\" is more likely than \"mujhe main\", let us see if it is reflected in the same\n",
    "a = calc_prob(['mujhe'], 'pata')\n",
    "b = calc_prob(['mujhe'], 'main')\n",
    "c = calc_prob(['mujhe'], 'nahin')\n",
    "print(f\"Probability of phrase a occuring is {a:.6f}\\nProbability of phrase b occuring is {b:.6f}\\nProbability of phrase c occuring os {c:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1768566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  now we have to find the most probable word for a previous_n_gram_token\n",
    "def max_prob(previous_n_gram, \n",
    "             vocab = closed_vocab, n_gram_counts=n_gram_counts, n_plus_one_gram_counts=n_plus_one_gram_counts,\n",
    "             vocab_size=len(closed_vocab), k=1.0, best_n_words = 5):\n",
    "    best_words = {}\n",
    "    all_words = list(vocab.keys()) + ['<unk>', '<e>']\n",
    "    probs = [calc_prob(previous_n_gram, word) for word in all_words] \n",
    "    indices = np.argsort(probs)\n",
    "    indices = indices[::-1]\n",
    "    # best n words\n",
    "    for i in range(best_n_words):\n",
    "        best_words[all_words[indices[i]]] = probs[indices[i]]\n",
    "    \n",
    "    return best_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4a03c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bhi': 0.0031179138321995466,\n",
       " '<e>': 0.002551020408163265,\n",
       " 'to': 0.0022675736961451248,\n",
       " 'toh': 0.0017006802721088435,\n",
       " 'laga': 0.0011337868480725624}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_words = max_prob(['mujhe'])\n",
    "best_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44897624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_sentence(initial_line, n = 2, length = 5):  # length means that we have to extend the intial sequence upto this more words\n",
    "    # preprocessing the statement first\n",
    "    prep_line = preprocessing(initial_line) # will return list of tokens\n",
    "    prep_line = [prep_line] # have to do this as the structure of unk_preprocessing demands this\n",
    "    prep_line = unk_preprocessing(prep_line, closed_vocab)\n",
    "    prep_line = prep_line[0]\n",
    "    prep_line = ['<s>']*(n-1) + prep_line # not appending the <e> token\n",
    "   # print(f\"prep_line = {prep_line}\\n\")\n",
    "    \n",
    "    # now i have to apply the n-gram predictive model\n",
    "    prev_n_gram = tuple(prep_line[-n+1:])  # picking up the last n-1 words \n",
    "    # as we know that in python our dictionary remains in the order the keys were inserted, so dictionary is already sorted\n",
    "    temp = initial_line\n",
    "    for i in range(length):\n",
    "        best_words = max_prob(prev_n_gram, best_n_words=3)\n",
    "        #print(f\"prev_n_gram = {prev_n_gram}\\nbest_words = {best_words}\\n\\n\\n\")\n",
    "        for word, prob in best_words.items():\n",
    "            if i==length-1:  # it is the last word, then we can end out at anyth\n",
    "                temp+= \" \"  + word\n",
    "                prev_n_gram = prev_n_gram[1:] + (word, )\n",
    "                break\n",
    "            elif word!='<unk>' and word!='<e>':\n",
    "                #print(\"word = \", word)\n",
    "                temp+= \" \" + word\n",
    "                prev_n_gram = prev_n_gram[1:] + (word, )\n",
    "                break\n",
    "                \n",
    "    return temp   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8d48281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closed_vocab['lawde']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d8e75ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bsdk tune bataya nhi hai ? <e>'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_sentence(\"bsdk\", length=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0385f148",
   "metadata": {},
   "source": [
    "### ***Log Perplexity Score***\n",
    "- It is a metric which have a meaning that how likely is the following sentence to occur.\n",
    "- Lower the value of the metric implies that great our model is doing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "45e29b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_perplexity(string, n=2):\n",
    "    prep_line = preprocessing(string) # will return list of tokens\n",
    "    prep_line = [prep_line] # have to do this as the structure of unk_preprocessing demands this\n",
    "    prep_line = unk_preprocessing(prep_line, closed_vocab)\n",
    "    prep_line = prep_line[0]\n",
    "    prep_line = ['<s>']*(n-1) + prep_line # not appending the <e> token\n",
    "    \n",
    "    score = 0\n",
    "    for i in range(0, len(prep_line)-n+1):\n",
    "        score+= m.log(calc_prob(prep_line[i:i+n-1], prep_line[i+n-1]))\n",
    "    score*= (-1/len(prep_line))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "65b6c10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.389004167882511\n",
      "6.2422945639222185\n"
     ]
    }
   ],
   "source": [
    "# the second sentence should have the higher perplexity \n",
    "print(log_perplexity('yr ye song sun na'))\n",
    "print(log_perplexity('mujhe main sun song na'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45b1ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126e16ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb2d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea78450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2cf45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
