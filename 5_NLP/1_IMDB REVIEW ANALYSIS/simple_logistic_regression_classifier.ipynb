{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b0a78cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from time import time \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef97afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "punct = string.punctuation\n",
    "stopwords_english = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d07f0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first think another Disney movie, might good, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big fan Stephen King's work, film made even gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truly enjoyed film. acting terrific plot. Jeff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>memory \"The Last Hunt\" stuck since saw 1956 13...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shakespeare fan, appreciate Ken Branagh done b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>privilege watching Scarface big screen beautif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>real classic. shipload sailors trying get town...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Serials short subjects originally shown theate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  polarity\n",
       "0  first think another Disney movie, might good, ...         1\n",
       "1  Put aside Dr. House repeat missed, Desperate H...         0\n",
       "2  big fan Stephen King's work, film made even gr...         1\n",
       "3  watched horrid thing TV. Needless say one movi...         0\n",
       "4  truly enjoyed film. acting terrific plot. Jeff...         1\n",
       "5  memory \"The Last Hunt\" stuck since saw 1956 13...         1\n",
       "6  Shakespeare fan, appreciate Ken Branagh done b...         0\n",
       "7  privilege watching Scarface big screen beautif...         1\n",
       "8  real classic. shipload sailors trying get town...         1\n",
       "9  Serials short subjects originally shown theate...         1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('imdb_complete.csv')\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1714fc53",
   "metadata": {},
   "source": [
    "## *TEXT PREPROCESSING*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f094fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70d44d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayush\\AppData\\Local\\Temp/ipykernel_19528/3147475427.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['msg_lower'] = test_data['text'].apply(lambda x:x.lower())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>msg_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first think another Disney movie, might good, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>first think another disney movie, might good, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
       "      <td>0</td>\n",
       "      <td>put aside dr. house repeat missed, desperate h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big fan Stephen King's work, film made even gr...</td>\n",
       "      <td>1</td>\n",
       "      <td>big fan stephen king's work, film made even gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
       "      <td>0</td>\n",
       "      <td>watched horrid thing tv. needless say one movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truly enjoyed film. acting terrific plot. Jeff...</td>\n",
       "      <td>1</td>\n",
       "      <td>truly enjoyed film. acting terrific plot. jeff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>memory \"The Last Hunt\" stuck since saw 1956 13...</td>\n",
       "      <td>1</td>\n",
       "      <td>memory \"the last hunt\" stuck since saw 1956 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shakespeare fan, appreciate Ken Branagh done b...</td>\n",
       "      <td>0</td>\n",
       "      <td>shakespeare fan, appreciate ken branagh done b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>privilege watching Scarface big screen beautif...</td>\n",
       "      <td>1</td>\n",
       "      <td>privilege watching scarface big screen beautif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>real classic. shipload sailors trying get town...</td>\n",
       "      <td>1</td>\n",
       "      <td>real classic. shipload sailors trying get town...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Serials short subjects originally shown theate...</td>\n",
       "      <td>1</td>\n",
       "      <td>serials short subjects originally shown theate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  polarity  \\\n",
       "0  first think another Disney movie, might good, ...         1   \n",
       "1  Put aside Dr. House repeat missed, Desperate H...         0   \n",
       "2  big fan Stephen King's work, film made even gr...         1   \n",
       "3  watched horrid thing TV. Needless say one movi...         0   \n",
       "4  truly enjoyed film. acting terrific plot. Jeff...         1   \n",
       "5  memory \"The Last Hunt\" stuck since saw 1956 13...         1   \n",
       "6  Shakespeare fan, appreciate Ken Branagh done b...         0   \n",
       "7  privilege watching Scarface big screen beautif...         1   \n",
       "8  real classic. shipload sailors trying get town...         1   \n",
       "9  Serials short subjects originally shown theate...         1   \n",
       "\n",
       "                                           msg_lower  \n",
       "0  first think another disney movie, might good, ...  \n",
       "1  put aside dr. house repeat missed, desperate h...  \n",
       "2  big fan stephen king's work, film made even gr...  \n",
       "3  watched horrid thing tv. needless say one movi...  \n",
       "4  truly enjoyed film. acting terrific plot. jeff...  \n",
       "5  memory \"the last hunt\" stuck since saw 1956 13...  \n",
       "6  shakespeare fan, appreciate ken branagh done b...  \n",
       "7  privilege watching scarface big screen beautif...  \n",
       "8  real classic. shipload sailors trying get town...  \n",
       "9  serials short subjects originally shown theate...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['msg_lower'] = test_data['text'].apply(lambda x:x.lower())\n",
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a74a0968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayush\\AppData\\Local\\Temp/ipykernel_19528/240293099.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['tokenized'] = test_data['msg_lower'].apply(lambda x:tokenization(x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>msg_lower</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first think another Disney movie, might good, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>first think another disney movie, might good, ...</td>\n",
       "      <td>[first, think, another, disney, movie, , might...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
       "      <td>0</td>\n",
       "      <td>put aside dr. house repeat missed, desperate h...</td>\n",
       "      <td>[put, aside, dr, , house, repeat, missed, , de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big fan Stephen King's work, film made even gr...</td>\n",
       "      <td>1</td>\n",
       "      <td>big fan stephen king's work, film made even gr...</td>\n",
       "      <td>[big, fan, stephen, king's, work, , film, made...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
       "      <td>0</td>\n",
       "      <td>watched horrid thing tv. needless say one movi...</td>\n",
       "      <td>[watched, horrid, thing, tv, , needless, say, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truly enjoyed film. acting terrific plot. Jeff...</td>\n",
       "      <td>1</td>\n",
       "      <td>truly enjoyed film. acting terrific plot. jeff...</td>\n",
       "      <td>[truly, enjoyed, film, , acting, terrific, plo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>memory \"The Last Hunt\" stuck since saw 1956 13...</td>\n",
       "      <td>1</td>\n",
       "      <td>memory \"the last hunt\" stuck since saw 1956 13...</td>\n",
       "      <td>[memory, \"the, last, hunt\", stuck, since, saw,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shakespeare fan, appreciate Ken Branagh done b...</td>\n",
       "      <td>0</td>\n",
       "      <td>shakespeare fan, appreciate ken branagh done b...</td>\n",
       "      <td>[shakespeare, fan, , appreciate, ken, branagh,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>privilege watching Scarface big screen beautif...</td>\n",
       "      <td>1</td>\n",
       "      <td>privilege watching scarface big screen beautif...</td>\n",
       "      <td>[privilege, watching, scarface, big, screen, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>real classic. shipload sailors trying get town...</td>\n",
       "      <td>1</td>\n",
       "      <td>real classic. shipload sailors trying get town...</td>\n",
       "      <td>[real, classic, , shipload, sailors, trying, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Serials short subjects originally shown theate...</td>\n",
       "      <td>1</td>\n",
       "      <td>serials short subjects originally shown theate...</td>\n",
       "      <td>[serials, short, subjects, originally, shown, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  polarity  \\\n",
       "0  first think another Disney movie, might good, ...         1   \n",
       "1  Put aside Dr. House repeat missed, Desperate H...         0   \n",
       "2  big fan Stephen King's work, film made even gr...         1   \n",
       "3  watched horrid thing TV. Needless say one movi...         0   \n",
       "4  truly enjoyed film. acting terrific plot. Jeff...         1   \n",
       "5  memory \"The Last Hunt\" stuck since saw 1956 13...         1   \n",
       "6  Shakespeare fan, appreciate Ken Branagh done b...         0   \n",
       "7  privilege watching Scarface big screen beautif...         1   \n",
       "8  real classic. shipload sailors trying get town...         1   \n",
       "9  Serials short subjects originally shown theate...         1   \n",
       "\n",
       "                                           msg_lower  \\\n",
       "0  first think another disney movie, might good, ...   \n",
       "1  put aside dr. house repeat missed, desperate h...   \n",
       "2  big fan stephen king's work, film made even gr...   \n",
       "3  watched horrid thing tv. needless say one movi...   \n",
       "4  truly enjoyed film. acting terrific plot. jeff...   \n",
       "5  memory \"the last hunt\" stuck since saw 1956 13...   \n",
       "6  shakespeare fan, appreciate ken branagh done b...   \n",
       "7  privilege watching scarface big screen beautif...   \n",
       "8  real classic. shipload sailors trying get town...   \n",
       "9  serials short subjects originally shown theate...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [first, think, another, disney, movie, , might...  \n",
       "1  [put, aside, dr, , house, repeat, missed, , de...  \n",
       "2  [big, fan, stephen, king's, work, , film, made...  \n",
       "3  [watched, horrid, thing, tv, , needless, say, ...  \n",
       "4  [truly, enjoyed, film, , acting, terrific, plo...  \n",
       "5  [memory, \"the, last, hunt\", stuck, since, saw,...  \n",
       "6  [shakespeare, fan, , appreciate, ken, branagh,...  \n",
       "7  [privilege, watching, scarface, big, screen, b...  \n",
       "8  [real, classic, , shipload, sailors, trying, g...  \n",
       "9  [serials, short, subjects, originally, shown, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenization(string):\n",
    "    tokens = re.split('\\s|(?<!\\d)[,.](?!\\d)', string)\n",
    "    return tokens\n",
    "#######################################\n",
    "test_data['tokenized'] = test_data['msg_lower'].apply(lambda x:tokenization(x))\n",
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "520d4041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayush\\AppData\\Local\\Temp/ipykernel_19528/4228114512.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['clean_tokens'] = test_data['tokenized'].apply(lambda x:clean_stopwords(x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>msg_lower</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>clean_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first think another Disney movie, might good, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>first think another disney movie, might good, ...</td>\n",
       "      <td>[first, think, another, disney, movie, , might...</td>\n",
       "      <td>[first, think, another, disney, movie, , might...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
       "      <td>0</td>\n",
       "      <td>put aside dr. house repeat missed, desperate h...</td>\n",
       "      <td>[put, aside, dr, , house, repeat, missed, , de...</td>\n",
       "      <td>[put, aside, dr, , house, repeat, missed, , de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big fan Stephen King's work, film made even gr...</td>\n",
       "      <td>1</td>\n",
       "      <td>big fan stephen king's work, film made even gr...</td>\n",
       "      <td>[big, fan, stephen, king's, work, , film, made...</td>\n",
       "      <td>[big, fan, stephen, king's, work, , film, made...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
       "      <td>0</td>\n",
       "      <td>watched horrid thing tv. needless say one movi...</td>\n",
       "      <td>[watched, horrid, thing, tv, , needless, say, ...</td>\n",
       "      <td>[watched, horrid, thing, tv, , needless, say, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truly enjoyed film. acting terrific plot. Jeff...</td>\n",
       "      <td>1</td>\n",
       "      <td>truly enjoyed film. acting terrific plot. jeff...</td>\n",
       "      <td>[truly, enjoyed, film, , acting, terrific, plo...</td>\n",
       "      <td>[truly, enjoyed, film, , acting, terrific, plo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>memory \"The Last Hunt\" stuck since saw 1956 13...</td>\n",
       "      <td>1</td>\n",
       "      <td>memory \"the last hunt\" stuck since saw 1956 13...</td>\n",
       "      <td>[memory, \"the, last, hunt\", stuck, since, saw,...</td>\n",
       "      <td>[memory, \"the, last, hunt\", stuck, since, saw,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shakespeare fan, appreciate Ken Branagh done b...</td>\n",
       "      <td>0</td>\n",
       "      <td>shakespeare fan, appreciate ken branagh done b...</td>\n",
       "      <td>[shakespeare, fan, , appreciate, ken, branagh,...</td>\n",
       "      <td>[shakespeare, fan, , appreciate, ken, branagh,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>privilege watching Scarface big screen beautif...</td>\n",
       "      <td>1</td>\n",
       "      <td>privilege watching scarface big screen beautif...</td>\n",
       "      <td>[privilege, watching, scarface, big, screen, b...</td>\n",
       "      <td>[privilege, watching, scarface, big, screen, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>real classic. shipload sailors trying get town...</td>\n",
       "      <td>1</td>\n",
       "      <td>real classic. shipload sailors trying get town...</td>\n",
       "      <td>[real, classic, , shipload, sailors, trying, g...</td>\n",
       "      <td>[real, classic, , shipload, sailors, trying, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Serials short subjects originally shown theate...</td>\n",
       "      <td>1</td>\n",
       "      <td>serials short subjects originally shown theate...</td>\n",
       "      <td>[serials, short, subjects, originally, shown, ...</td>\n",
       "      <td>[serials, short, subjects, originally, shown, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  polarity  \\\n",
       "0  first think another Disney movie, might good, ...         1   \n",
       "1  Put aside Dr. House repeat missed, Desperate H...         0   \n",
       "2  big fan Stephen King's work, film made even gr...         1   \n",
       "3  watched horrid thing TV. Needless say one movi...         0   \n",
       "4  truly enjoyed film. acting terrific plot. Jeff...         1   \n",
       "5  memory \"The Last Hunt\" stuck since saw 1956 13...         1   \n",
       "6  Shakespeare fan, appreciate Ken Branagh done b...         0   \n",
       "7  privilege watching Scarface big screen beautif...         1   \n",
       "8  real classic. shipload sailors trying get town...         1   \n",
       "9  Serials short subjects originally shown theate...         1   \n",
       "\n",
       "                                           msg_lower  \\\n",
       "0  first think another disney movie, might good, ...   \n",
       "1  put aside dr. house repeat missed, desperate h...   \n",
       "2  big fan stephen king's work, film made even gr...   \n",
       "3  watched horrid thing tv. needless say one movi...   \n",
       "4  truly enjoyed film. acting terrific plot. jeff...   \n",
       "5  memory \"the last hunt\" stuck since saw 1956 13...   \n",
       "6  shakespeare fan, appreciate ken branagh done b...   \n",
       "7  privilege watching scarface big screen beautif...   \n",
       "8  real classic. shipload sailors trying get town...   \n",
       "9  serials short subjects originally shown theate...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [first, think, another, disney, movie, , might...   \n",
       "1  [put, aside, dr, , house, repeat, missed, , de...   \n",
       "2  [big, fan, stephen, king's, work, , film, made...   \n",
       "3  [watched, horrid, thing, tv, , needless, say, ...   \n",
       "4  [truly, enjoyed, film, , acting, terrific, plo...   \n",
       "5  [memory, \"the, last, hunt\", stuck, since, saw,...   \n",
       "6  [shakespeare, fan, , appreciate, ken, branagh,...   \n",
       "7  [privilege, watching, scarface, big, screen, b...   \n",
       "8  [real, classic, , shipload, sailors, trying, g...   \n",
       "9  [serials, short, subjects, originally, shown, ...   \n",
       "\n",
       "                                        clean_tokens  \n",
       "0  [first, think, another, disney, movie, , might...  \n",
       "1  [put, aside, dr, , house, repeat, missed, , de...  \n",
       "2  [big, fan, stephen, king's, work, , film, made...  \n",
       "3  [watched, horrid, thing, tv, , needless, say, ...  \n",
       "4  [truly, enjoyed, film, , acting, terrific, plo...  \n",
       "5  [memory, \"the, last, hunt\", stuck, since, saw,...  \n",
       "6  [shakespeare, fan, , appreciate, ken, branagh,...  \n",
       "7  [privilege, watching, scarface, big, screen, b...  \n",
       "8  [real, classic, , shipload, sailors, trying, g...  \n",
       "9  [serials, short, subjects, originally, shown, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_stopwords(token_list):\n",
    "    clean_token = []\n",
    "    for word in token_list:\n",
    "        if word not in stopwords_english:\n",
    "            clean_token.append(word)\n",
    "    return clean_token\n",
    "################################################\n",
    "test_data['clean_tokens'] = test_data['tokenized'].apply(lambda x:clean_stopwords(x))\n",
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2e82a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayush\\AppData\\Local\\Temp/ipykernel_19528/4184230196.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['stemmed_tokens'] = test_data['clean_tokens'].apply(lambda x:stem(x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>msg_lower</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first think another Disney movie, might good, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>first think another disney movie, might good, ...</td>\n",
       "      <td>[first, think, another, disney, movie, , might...</td>\n",
       "      <td>[first, think, another, disney, movie, , might...</td>\n",
       "      <td>[first, think, anoth, disney, movi, , might, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
       "      <td>0</td>\n",
       "      <td>put aside dr. house repeat missed, desperate h...</td>\n",
       "      <td>[put, aside, dr, , house, repeat, missed, , de...</td>\n",
       "      <td>[put, aside, dr, , house, repeat, missed, , de...</td>\n",
       "      <td>[put, asid, dr, , hous, repeat, miss, , desper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big fan Stephen King's work, film made even gr...</td>\n",
       "      <td>1</td>\n",
       "      <td>big fan stephen king's work, film made even gr...</td>\n",
       "      <td>[big, fan, stephen, king's, work, , film, made...</td>\n",
       "      <td>[big, fan, stephen, king's, work, , film, made...</td>\n",
       "      <td>[big, fan, stephen, king', work, , film, made,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
       "      <td>0</td>\n",
       "      <td>watched horrid thing tv. needless say one movi...</td>\n",
       "      <td>[watched, horrid, thing, tv, , needless, say, ...</td>\n",
       "      <td>[watched, horrid, thing, tv, , needless, say, ...</td>\n",
       "      <td>[watch, horrid, thing, tv, , needless, say, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truly enjoyed film. acting terrific plot. Jeff...</td>\n",
       "      <td>1</td>\n",
       "      <td>truly enjoyed film. acting terrific plot. jeff...</td>\n",
       "      <td>[truly, enjoyed, film, , acting, terrific, plo...</td>\n",
       "      <td>[truly, enjoyed, film, , acting, terrific, plo...</td>\n",
       "      <td>[truli, enjoy, film, , act, terrif, plot, , je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>memory \"The Last Hunt\" stuck since saw 1956 13...</td>\n",
       "      <td>1</td>\n",
       "      <td>memory \"the last hunt\" stuck since saw 1956 13...</td>\n",
       "      <td>[memory, \"the, last, hunt\", stuck, since, saw,...</td>\n",
       "      <td>[memory, \"the, last, hunt\", stuck, since, saw,...</td>\n",
       "      <td>[memori, \"the, last, hunt\", stuck, sinc, saw, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shakespeare fan, appreciate Ken Branagh done b...</td>\n",
       "      <td>0</td>\n",
       "      <td>shakespeare fan, appreciate ken branagh done b...</td>\n",
       "      <td>[shakespeare, fan, , appreciate, ken, branagh,...</td>\n",
       "      <td>[shakespeare, fan, , appreciate, ken, branagh,...</td>\n",
       "      <td>[shakespear, fan, , appreci, ken, branagh, don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>privilege watching Scarface big screen beautif...</td>\n",
       "      <td>1</td>\n",
       "      <td>privilege watching scarface big screen beautif...</td>\n",
       "      <td>[privilege, watching, scarface, big, screen, b...</td>\n",
       "      <td>[privilege, watching, scarface, big, screen, b...</td>\n",
       "      <td>[privileg, watch, scarfac, big, screen, beauti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>real classic. shipload sailors trying get town...</td>\n",
       "      <td>1</td>\n",
       "      <td>real classic. shipload sailors trying get town...</td>\n",
       "      <td>[real, classic, , shipload, sailors, trying, g...</td>\n",
       "      <td>[real, classic, , shipload, sailors, trying, g...</td>\n",
       "      <td>[real, classic, , shipload, sailor, tri, get, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Serials short subjects originally shown theate...</td>\n",
       "      <td>1</td>\n",
       "      <td>serials short subjects originally shown theate...</td>\n",
       "      <td>[serials, short, subjects, originally, shown, ...</td>\n",
       "      <td>[serials, short, subjects, originally, shown, ...</td>\n",
       "      <td>[serial, short, subject, origin, shown, theate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  polarity  \\\n",
       "0  first think another Disney movie, might good, ...         1   \n",
       "1  Put aside Dr. House repeat missed, Desperate H...         0   \n",
       "2  big fan Stephen King's work, film made even gr...         1   \n",
       "3  watched horrid thing TV. Needless say one movi...         0   \n",
       "4  truly enjoyed film. acting terrific plot. Jeff...         1   \n",
       "5  memory \"The Last Hunt\" stuck since saw 1956 13...         1   \n",
       "6  Shakespeare fan, appreciate Ken Branagh done b...         0   \n",
       "7  privilege watching Scarface big screen beautif...         1   \n",
       "8  real classic. shipload sailors trying get town...         1   \n",
       "9  Serials short subjects originally shown theate...         1   \n",
       "\n",
       "                                           msg_lower  \\\n",
       "0  first think another disney movie, might good, ...   \n",
       "1  put aside dr. house repeat missed, desperate h...   \n",
       "2  big fan stephen king's work, film made even gr...   \n",
       "3  watched horrid thing tv. needless say one movi...   \n",
       "4  truly enjoyed film. acting terrific plot. jeff...   \n",
       "5  memory \"the last hunt\" stuck since saw 1956 13...   \n",
       "6  shakespeare fan, appreciate ken branagh done b...   \n",
       "7  privilege watching scarface big screen beautif...   \n",
       "8  real classic. shipload sailors trying get town...   \n",
       "9  serials short subjects originally shown theate...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [first, think, another, disney, movie, , might...   \n",
       "1  [put, aside, dr, , house, repeat, missed, , de...   \n",
       "2  [big, fan, stephen, king's, work, , film, made...   \n",
       "3  [watched, horrid, thing, tv, , needless, say, ...   \n",
       "4  [truly, enjoyed, film, , acting, terrific, plo...   \n",
       "5  [memory, \"the, last, hunt\", stuck, since, saw,...   \n",
       "6  [shakespeare, fan, , appreciate, ken, branagh,...   \n",
       "7  [privilege, watching, scarface, big, screen, b...   \n",
       "8  [real, classic, , shipload, sailors, trying, g...   \n",
       "9  [serials, short, subjects, originally, shown, ...   \n",
       "\n",
       "                                        clean_tokens  \\\n",
       "0  [first, think, another, disney, movie, , might...   \n",
       "1  [put, aside, dr, , house, repeat, missed, , de...   \n",
       "2  [big, fan, stephen, king's, work, , film, made...   \n",
       "3  [watched, horrid, thing, tv, , needless, say, ...   \n",
       "4  [truly, enjoyed, film, , acting, terrific, plo...   \n",
       "5  [memory, \"the, last, hunt\", stuck, since, saw,...   \n",
       "6  [shakespeare, fan, , appreciate, ken, branagh,...   \n",
       "7  [privilege, watching, scarface, big, screen, b...   \n",
       "8  [real, classic, , shipload, sailors, trying, g...   \n",
       "9  [serials, short, subjects, originally, shown, ...   \n",
       "\n",
       "                                      stemmed_tokens  \n",
       "0  [first, think, anoth, disney, movi, , might, g...  \n",
       "1  [put, asid, dr, , hous, repeat, miss, , desper...  \n",
       "2  [big, fan, stephen, king', work, , film, made,...  \n",
       "3  [watch, horrid, thing, tv, , needless, say, on...  \n",
       "4  [truli, enjoy, film, , act, terrif, plot, , je...  \n",
       "5  [memori, \"the, last, hunt\", stuck, sinc, saw, ...  \n",
       "6  [shakespear, fan, , appreci, ken, branagh, don...  \n",
       "7  [privileg, watch, scarfac, big, screen, beauti...  \n",
       "8  [real, classic, , shipload, sailor, tri, get, ...  \n",
       "9  [serial, short, subject, origin, shown, theate...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stem(clean_tokens):\n",
    "    stemmed_tokens = []\n",
    "    for word in clean_tokens:\n",
    "        stemmed_tokens.append(stemmer.stem(word))\n",
    "    return stemmed_tokens\n",
    "#######################################\n",
    "test_data['stemmed_tokens'] = test_data['clean_tokens'].apply(lambda x:stem(x))\n",
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc86cd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first think another Disney movie, might good, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[first, think, anoth, disney, movi, might, goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
       "      <td>0</td>\n",
       "      <td>[put, asid, dr, hous, repeat, miss, desper, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big fan Stephen King's work, film made even gr...</td>\n",
       "      <td>1</td>\n",
       "      <td>[big, fan, stephen, king', work, film, made, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[watch, horrid, thing, tv, needless, say, one,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truly enjoyed film. acting terrific plot. Jeff...</td>\n",
       "      <td>1</td>\n",
       "      <td>[truli, enjoy, film, act, terrif, plot, jeff, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>memory \"The Last Hunt\" stuck since saw 1956 13...</td>\n",
       "      <td>1</td>\n",
       "      <td>[memori, \"the, last, hunt\", stuck, sinc, saw, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shakespeare fan, appreciate Ken Branagh done b...</td>\n",
       "      <td>0</td>\n",
       "      <td>[shakespear, fan, appreci, ken, branagh, done,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>privilege watching Scarface big screen beautif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[privileg, watch, scarfac, big, screen, beauti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>real classic. shipload sailors trying get town...</td>\n",
       "      <td>1</td>\n",
       "      <td>[real, classic, shipload, sailor, tri, get, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Serials short subjects originally shown theate...</td>\n",
       "      <td>1</td>\n",
       "      <td>[serial, short, subject, origin, shown, theate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  polarity  \\\n",
       "0  first think another Disney movie, might good, ...         1   \n",
       "1  Put aside Dr. House repeat missed, Desperate H...         0   \n",
       "2  big fan Stephen King's work, film made even gr...         1   \n",
       "3  watched horrid thing TV. Needless say one movi...         0   \n",
       "4  truly enjoyed film. acting terrific plot. Jeff...         1   \n",
       "5  memory \"The Last Hunt\" stuck since saw 1956 13...         1   \n",
       "6  Shakespeare fan, appreciate Ken Branagh done b...         0   \n",
       "7  privilege watching Scarface big screen beautif...         1   \n",
       "8  real classic. shipload sailors trying get town...         1   \n",
       "9  Serials short subjects originally shown theate...         1   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  [first, think, anoth, disney, movi, might, goo...  \n",
       "1  [put, asid, dr, hous, repeat, miss, desper, ho...  \n",
       "2  [big, fan, stephen, king', work, film, made, e...  \n",
       "3  [watch, horrid, thing, tv, needless, say, one,...  \n",
       "4  [truli, enjoy, film, act, terrif, plot, jeff, ...  \n",
       "5  [memori, \"the, last, hunt\", stuck, sinc, saw, ...  \n",
       "6  [shakespear, fan, appreci, ken, branagh, done,...  \n",
       "7  [privileg, watch, scarfac, big, screen, beauti...  \n",
       "8  [real, classic, shipload, sailor, tri, get, to...  \n",
       "9  [serial, short, subject, origin, shown, theate...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessing(string, stopwords, stemmer):\n",
    "    '''We can do all the preprocessing in just one step by creating a pipeline\n",
    "    First, we have to make all the words in lowercase,\n",
    "    then we have to tokenize the string,\n",
    "    then we have to remove stopwords and \n",
    "    finally we have to stem all the words.\n",
    "    This is how it will be ready to be analyzed further'''\n",
    "    string  = string.lower()\n",
    "    tokens = re.split('\\s|(?<!\\d)[,.](?!\\d)', string)\n",
    "    clean_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stopwords:\n",
    "            clean_tokens.append(word)\n",
    "    \n",
    "    stemmed_words = []\n",
    "    for word in clean_tokens:\n",
    "        stemmed_words.append(stemmer.stem(word))\n",
    "    \n",
    "    preprocessed_array = []\n",
    "    for word in stemmed_words:\n",
    "        if word!='':\n",
    "            preprocessed_array.append(word)\n",
    "            \n",
    "    return preprocessed_array\n",
    "#########################################\n",
    "dataset['preprocessed'] = dataset['text'].apply(lambda x:preprocessing(x, stopwords_english, stemmer))    \n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c83bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_frequency(processed_strings, polarities):\n",
    "    vocab = {}\n",
    "    for string, polarity in zip(processed_strings, polarities):\n",
    "        for word in string:\n",
    "            pair = (word, polarity)\n",
    "            if pair in vocab:\n",
    "                vocab[pair]+=1\n",
    "            else:\n",
    "                vocab[pair] = 1\n",
    "    return vocab\n",
    "#####################################\n",
    "vocab = build_frequency(dataset['preprocessed'], dataset['polarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0668a84",
   "metadata": {},
   "source": [
    "#### Everything is ready, we have preprocesed our sentiment strings, and we have built the vocabulary with the help of the text corpus, now we have to make the data which is in trainable form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7658c88",
   "metadata": {},
   "source": [
    "## *Forming the trainable data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55f21ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>pos_counts</th>\n",
       "      <th>neg_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first think another Disney movie, might good, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[first, think, anoth, disney, movi, might, goo...</td>\n",
       "      <td>71546</td>\n",
       "      <td>74604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
       "      <td>0</td>\n",
       "      <td>[put, asid, dr, hous, repeat, miss, desper, ho...</td>\n",
       "      <td>115914</td>\n",
       "      <td>124907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big fan Stephen King's work, film made even gr...</td>\n",
       "      <td>1</td>\n",
       "      <td>[big, fan, stephen, king', work, film, made, e...</td>\n",
       "      <td>233171</td>\n",
       "      <td>234576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[watch, horrid, thing, tv, needless, say, one,...</td>\n",
       "      <td>133129</td>\n",
       "      <td>146641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truly enjoyed film. acting terrific plot. Jeff...</td>\n",
       "      <td>1</td>\n",
       "      <td>[truli, enjoy, film, act, terrif, plot, jeff, ...</td>\n",
       "      <td>72363</td>\n",
       "      <td>71899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>memory \"The Last Hunt\" stuck since saw 1956 13...</td>\n",
       "      <td>1</td>\n",
       "      <td>[memori, \"the, last, hunt\", stuck, sinc, saw, ...</td>\n",
       "      <td>34010</td>\n",
       "      <td>35889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shakespeare fan, appreciate Ken Branagh done b...</td>\n",
       "      <td>0</td>\n",
       "      <td>[shakespear, fan, appreci, ken, branagh, done,...</td>\n",
       "      <td>23933</td>\n",
       "      <td>24215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>privilege watching Scarface big screen beautif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[privileg, watch, scarfac, big, screen, beauti...</td>\n",
       "      <td>290516</td>\n",
       "      <td>280749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>real classic. shipload sailors trying get town...</td>\n",
       "      <td>1</td>\n",
       "      <td>[real, classic, shipload, sailor, tri, get, to...</td>\n",
       "      <td>19007</td>\n",
       "      <td>18584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Serials short subjects originally shown theate...</td>\n",
       "      <td>1</td>\n",
       "      <td>[serial, short, subject, origin, shown, theate...</td>\n",
       "      <td>204664</td>\n",
       "      <td>202230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  polarity  \\\n",
       "0  first think another Disney movie, might good, ...         1   \n",
       "1  Put aside Dr. House repeat missed, Desperate H...         0   \n",
       "2  big fan Stephen King's work, film made even gr...         1   \n",
       "3  watched horrid thing TV. Needless say one movi...         0   \n",
       "4  truly enjoyed film. acting terrific plot. Jeff...         1   \n",
       "5  memory \"The Last Hunt\" stuck since saw 1956 13...         1   \n",
       "6  Shakespeare fan, appreciate Ken Branagh done b...         0   \n",
       "7  privilege watching Scarface big screen beautif...         1   \n",
       "8  real classic. shipload sailors trying get town...         1   \n",
       "9  Serials short subjects originally shown theate...         1   \n",
       "\n",
       "                                        preprocessed  pos_counts  neg_counts  \n",
       "0  [first, think, anoth, disney, movi, might, goo...       71546       74604  \n",
       "1  [put, asid, dr, hous, repeat, miss, desper, ho...      115914      124907  \n",
       "2  [big, fan, stephen, king', work, film, made, e...      233171      234576  \n",
       "3  [watch, horrid, thing, tv, needless, say, one,...      133129      146641  \n",
       "4  [truli, enjoy, film, act, terrif, plot, jeff, ...       72363       71899  \n",
       "5  [memori, \"the, last, hunt\", stuck, sinc, saw, ...       34010       35889  \n",
       "6  [shakespear, fan, appreci, ken, branagh, done,...       23933       24215  \n",
       "7  [privileg, watch, scarfac, big, screen, beauti...      290516      280749  \n",
       "8  [real, classic, shipload, sailor, tri, get, to...       19007       18584  \n",
       "9  [serial, short, subject, origin, shown, theate...      204664      202230  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_counts(string, vocab):\n",
    "    pos = 0\n",
    "    for word in string:\n",
    "        if (word,1) in vocab:\n",
    "            pos+= vocab[(word, 1)]\n",
    "    return pos\n",
    "\n",
    "def neg_counts(string, vocab):\n",
    "    neg = 0\n",
    "    for word in string:\n",
    "        if (word, 0) in vocab:\n",
    "            neg+= vocab[(word, 0)]\n",
    "    return neg\n",
    "#######################################\n",
    "dataset['pos_counts'] = dataset['preprocessed'].apply(lambda x:pos_counts(x, vocab))\n",
    "dataset['neg_counts'] = dataset['preprocessed'].apply(lambda x:neg_counts(x, vocab))\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16cbef32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>pos_counts</th>\n",
       "      <th>neg_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>71546</td>\n",
       "      <td>74604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>115914</td>\n",
       "      <td>124907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>233171</td>\n",
       "      <td>234576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>133129</td>\n",
       "      <td>146641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>72363</td>\n",
       "      <td>71899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>34010</td>\n",
       "      <td>35889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>23933</td>\n",
       "      <td>24215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>290516</td>\n",
       "      <td>280749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>19007</td>\n",
       "      <td>18584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>204664</td>\n",
       "      <td>202230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity  pos_counts  neg_counts\n",
       "0         1       71546       74604\n",
       "1         0      115914      124907\n",
       "2         1      233171      234576\n",
       "3         0      133129      146641\n",
       "4         1       72363       71899\n",
       "5         1       34010       35889\n",
       "6         0       23933       24215\n",
       "7         1      290516      280749\n",
       "8         1       19007       18584\n",
       "9         1      204664      202230"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset.drop(['text', 'preprocessed'], axis = 1)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c478b",
   "metadata": {},
   "source": [
    "## *Now the trainable data is ready, we have  the make the training and test sets and make the dataloaders*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2efd5311",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mean = data['pos_counts'].mean()\n",
    "neg_mean = data['neg_counts'].mean()\n",
    "pos_std = data['pos_counts'].std()\n",
    "neg_std = data['neg_counts'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e84dc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>pos_counts</th>\n",
       "      <th>neg_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.672276</td>\n",
       "      <td>-0.663629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.252751</td>\n",
       "      <td>-0.199212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.855983</td>\n",
       "      <td>0.813294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.089973</td>\n",
       "      <td>0.001444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.664551</td>\n",
       "      <td>-0.688602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.027201</td>\n",
       "      <td>-1.021060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.122485</td>\n",
       "      <td>-1.128839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1.398213</td>\n",
       "      <td>1.239581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.169063</td>\n",
       "      <td>-1.180827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0.586432</td>\n",
       "      <td>0.514663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity  pos_counts  neg_counts\n",
       "0         1   -0.672276   -0.663629\n",
       "1         0   -0.252751   -0.199212\n",
       "2         1    0.855983    0.813294\n",
       "3         0   -0.089973    0.001444\n",
       "4         1   -0.664551   -0.688602\n",
       "5         1   -1.027201   -1.021060\n",
       "6         0   -1.122485   -1.128839\n",
       "7         1    1.398213    1.239581\n",
       "8         1   -1.169063   -1.180827\n",
       "9         1    0.586432    0.514663"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pos_counts']  = (data['pos_counts'] - pos_mean)/pos_std\n",
    "data['neg_counts'] = (data['neg_counts'] - neg_mean)/neg_std\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90539332",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.values\n",
    "data1 = data1.astype('float32')\n",
    "train_data, test_data = torch.utils.data.random_split(data1, (8000, 2000))\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ec9c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment_analysis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(sentiment_analysis, self).__init__()\n",
    "        self.linear1 = nn.Linear(2, 1).to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        op = self.linear1(x)\n",
    "        # not includeing sigmoid here as i will be defining the loss to be with logits\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8aa6eef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                    [-1, 1]               3\n",
      "================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = sentiment_analysis().to(device)\n",
    "summary(model, input_size=(2,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d11faa",
   "metadata": {},
   "source": [
    "## *Model has also been implemnted, now we have to define training and testing loops*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f6ccbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, lr = 128, 0.1\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "548835ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate(y_hat, y):\n",
    "    ans = 0\n",
    "    for i in range(y_hat.shape[0]):\n",
    "        if y_hat[i]>=0:\n",
    "            ans+= 1==y[i]\n",
    "        else:\n",
    "            ans+= 0==y[i]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81e6c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, batch_size = 128, print_every = 20):\n",
    "  total_loss = 0 # average of losses over all of the batches\n",
    "  accuracy = []\n",
    "  tic = time()\n",
    "\n",
    "  for i, batch in enumerate(train_dataloader,1):  # i represents number of batches starting from 1\n",
    "    model_input = (batch[:,1:]).to(device)\n",
    "    target = (batch[:,0]).to(device)\n",
    "\n",
    "    model_output = model(model_input)\n",
    "    model_output = model_output.reshape(target.shape[0])\n",
    "    loss = criterion(model_output, target)\n",
    "    total_loss+= loss.item()  # as it is just a tensor of 0 dimension, so converting it to a scalar by .item() method\n",
    "    # backpropoagation\n",
    "    optimizer.zero_grad() # making the gradients equal to zero if there are previously any\n",
    "    loss.backward()\n",
    "    optimizer.step()  # updating the gradients\n",
    "\n",
    "    accuracy.append(estimate(model_output, target)/batch_size) # accuracy over a batch\n",
    "\n",
    "    if i%print_every==0:   # should print something after every 'print_every' no. of batches are processed\n",
    "      print('Epoch [{}] ({}/{}), train_loss = {:.4f}, accuracy = {:.2f}, time = {:.2f} sec'.format(epoch, i, len(train_dataloader), loss.item(), sum(accuracy)/len(accuracy), time() - tic ))\n",
    "  return total_loss/len(train_dataloader) # returns average loss of all batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "204f3b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epochs, batch_size = 128):\n",
    "  total_loss = 0\n",
    "  accuracy = []\n",
    "  tic = time()\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    for i, batch in enumerate(test_dataloader, 0):\n",
    "      model_input = batch[:,1:].to(device)\n",
    "      target = (batch[:,0]).to(device)\n",
    "\n",
    "      model_output = model(model_input)\n",
    "      model_output = model_output.reshape(target.shape[0])\n",
    "      loss = criterion(model_output, target)\n",
    "      total_loss+= loss.item()\n",
    "    \n",
    "      acc = estimate(model_output, target)\n",
    "      accuracy.append(estimate(model_output, target)/batch_size) \n",
    "  print('Epoch: [{}], Test Loss: {:.4f}, Accuracy: {:.2f}, Time: {:.2f} sec'.format(\n",
    "        epoch, total_loss/len(test_dataloader), sum(accuracy)/len(accuracy), time()-tic\n",
    "    ))\n",
    "  return total_loss/len(test_dataloader) # Returning Average Testing Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "690cb530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] (20/63), train_loss = 0.7056, accuracy = 0.49, time = 0.09 sec\n",
      "Epoch [1] (40/63), train_loss = 0.7094, accuracy = 0.50, time = 0.17 sec\n",
      "Epoch [1] (60/63), train_loss = 0.6729, accuracy = 0.50, time = 0.27 sec\n",
      "Epoch: [1], Test Loss: 0.6874, Accuracy: 0.48, Time: 0.11 sec\n",
      "\n",
      "\n",
      "Epoch [2] (20/63), train_loss = 0.6843, accuracy = 0.55, time = 0.10 sec\n",
      "Epoch [2] (40/63), train_loss = 0.6893, accuracy = 0.55, time = 0.18 sec\n",
      "Epoch [2] (60/63), train_loss = 0.6495, accuracy = 0.56, time = 0.28 sec\n",
      "Epoch: [2], Test Loss: 0.6706, Accuracy: 0.59, Time: 0.13 sec\n",
      "\n",
      "\n",
      "Epoch [3] (20/63), train_loss = 0.6723, accuracy = 0.61, time = 0.08 sec\n",
      "Epoch [3] (40/63), train_loss = 0.6693, accuracy = 0.60, time = 0.17 sec\n",
      "Epoch [3] (60/63), train_loss = 0.6302, accuracy = 0.61, time = 0.27 sec\n",
      "Epoch: [3], Test Loss: 0.6566, Accuracy: 0.63, Time: 0.11 sec\n",
      "\n",
      "\n",
      "Epoch [4] (20/63), train_loss = 0.6622, accuracy = 0.64, time = 0.10 sec\n",
      "Epoch [4] (40/63), train_loss = 0.6517, accuracy = 0.64, time = 0.18 sec\n",
      "Epoch [4] (60/63), train_loss = 0.6143, accuracy = 0.64, time = 0.28 sec\n",
      "Epoch: [4], Test Loss: 0.6449, Accuracy: 0.66, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [5] (20/63), train_loss = 0.6538, accuracy = 0.66, time = 0.10 sec\n",
      "Epoch [5] (40/63), train_loss = 0.6367, accuracy = 0.66, time = 0.18 sec\n",
      "Epoch [5] (60/63), train_loss = 0.6012, accuracy = 0.66, time = 0.28 sec\n",
      "Epoch: [5], Test Loss: 0.6352, Accuracy: 0.66, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [6] (20/63), train_loss = 0.6468, accuracy = 0.67, time = 0.07 sec\n",
      "Epoch [6] (40/63), train_loss = 0.6240, accuracy = 0.67, time = 0.16 sec\n",
      "Epoch [6] (60/63), train_loss = 0.5903, accuracy = 0.67, time = 0.25 sec\n",
      "Epoch: [6], Test Loss: 0.6273, Accuracy: 0.67, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [7] (20/63), train_loss = 0.6411, accuracy = 0.67, time = 0.09 sec\n",
      "Epoch [7] (40/63), train_loss = 0.6131, accuracy = 0.68, time = 0.18 sec\n",
      "Epoch [7] (60/63), train_loss = 0.5811, accuracy = 0.68, time = 0.29 sec\n",
      "Epoch: [7], Test Loss: 0.6207, Accuracy: 0.68, Time: 0.14 sec\n",
      "\n",
      "\n",
      "Epoch [8] (20/63), train_loss = 0.6364, accuracy = 0.68, time = 0.10 sec\n",
      "Epoch [8] (40/63), train_loss = 0.6039, accuracy = 0.68, time = 0.21 sec\n",
      "Epoch [8] (60/63), train_loss = 0.5732, accuracy = 0.68, time = 0.30 sec\n",
      "Epoch: [8], Test Loss: 0.6152, Accuracy: 0.68, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [9] (20/63), train_loss = 0.6325, accuracy = 0.68, time = 0.12 sec\n",
      "Epoch [9] (40/63), train_loss = 0.5959, accuracy = 0.68, time = 0.20 sec\n",
      "Epoch [9] (60/63), train_loss = 0.5665, accuracy = 0.69, time = 0.28 sec\n",
      "Epoch: [9], Test Loss: 0.6106, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [10] (20/63), train_loss = 0.6292, accuracy = 0.68, time = 0.08 sec\n",
      "Epoch [10] (40/63), train_loss = 0.5890, accuracy = 0.69, time = 0.18 sec\n",
      "Epoch [10] (60/63), train_loss = 0.5607, accuracy = 0.69, time = 0.29 sec\n",
      "Epoch: [10], Test Loss: 0.6067, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [11] (20/63), train_loss = 0.6265, accuracy = 0.68, time = 0.12 sec\n",
      "Epoch [11] (40/63), train_loss = 0.5830, accuracy = 0.69, time = 0.22 sec\n",
      "Epoch [11] (60/63), train_loss = 0.5556, accuracy = 0.69, time = 0.32 sec\n",
      "Epoch: [11], Test Loss: 0.6034, Accuracy: 0.69, Time: 0.11 sec\n",
      "\n",
      "\n",
      "Epoch [12] (20/63), train_loss = 0.6243, accuracy = 0.68, time = 0.10 sec\n",
      "Epoch [12] (40/63), train_loss = 0.5777, accuracy = 0.69, time = 0.20 sec\n",
      "Epoch [12] (60/63), train_loss = 0.5512, accuracy = 0.69, time = 0.30 sec\n",
      "Epoch: [12], Test Loss: 0.6006, Accuracy: 0.69, Time: 0.13 sec\n",
      "\n",
      "\n",
      "Epoch [13] (20/63), train_loss = 0.6224, accuracy = 0.69, time = 0.13 sec\n",
      "Epoch [13] (40/63), train_loss = 0.5731, accuracy = 0.69, time = 0.23 sec\n",
      "Epoch [13] (60/63), train_loss = 0.5473, accuracy = 0.70, time = 0.34 sec\n",
      "Epoch: [13], Test Loss: 0.5982, Accuracy: 0.69, Time: 0.14 sec\n",
      "\n",
      "\n",
      "Epoch [14] (20/63), train_loss = 0.6209, accuracy = 0.68, time = 0.10 sec\n",
      "Epoch [14] (40/63), train_loss = 0.5691, accuracy = 0.69, time = 0.18 sec\n",
      "Epoch [14] (60/63), train_loss = 0.5438, accuracy = 0.70, time = 0.28 sec\n",
      "Epoch: [14], Test Loss: 0.5962, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [15] (20/63), train_loss = 0.6195, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [15] (40/63), train_loss = 0.5655, accuracy = 0.69, time = 0.17 sec\n",
      "Epoch [15] (60/63), train_loss = 0.5407, accuracy = 0.70, time = 0.28 sec\n",
      "Epoch: [15], Test Loss: 0.5944, Accuracy: 0.69, Time: 0.14 sec\n",
      "\n",
      "\n",
      "Epoch [16] (20/63), train_loss = 0.6184, accuracy = 0.69, time = 0.12 sec\n",
      "Epoch [16] (40/63), train_loss = 0.5623, accuracy = 0.69, time = 0.23 sec\n",
      "Epoch [16] (60/63), train_loss = 0.5380, accuracy = 0.70, time = 0.33 sec\n",
      "Epoch: [16], Test Loss: 0.5929, Accuracy: 0.69, Time: 0.14 sec\n",
      "\n",
      "\n",
      "Epoch [17] (20/63), train_loss = 0.6175, accuracy = 0.69, time = 0.09 sec\n",
      "Epoch [17] (40/63), train_loss = 0.5594, accuracy = 0.69, time = 0.19 sec\n",
      "Epoch [17] (60/63), train_loss = 0.5355, accuracy = 0.70, time = 0.31 sec\n",
      "Epoch: [17], Test Loss: 0.5916, Accuracy: 0.69, Time: 0.13 sec\n",
      "\n",
      "\n",
      "Epoch [18] (20/63), train_loss = 0.6168, accuracy = 0.69, time = 0.12 sec\n",
      "Epoch [18] (40/63), train_loss = 0.5568, accuracy = 0.70, time = 0.23 sec\n",
      "Epoch [18] (60/63), train_loss = 0.5333, accuracy = 0.70, time = 0.31 sec\n",
      "Epoch: [18], Test Loss: 0.5905, Accuracy: 0.69, Time: 0.11 sec\n",
      "\n",
      "\n",
      "Epoch [19] (20/63), train_loss = 0.6161, accuracy = 0.69, time = 0.09 sec\n",
      "Epoch [19] (40/63), train_loss = 0.5545, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [19] (60/63), train_loss = 0.5313, accuracy = 0.70, time = 0.29 sec\n",
      "Epoch: [19], Test Loss: 0.5895, Accuracy: 0.69, Time: 0.13 sec\n",
      "\n",
      "\n",
      "Epoch [20] (20/63), train_loss = 0.6156, accuracy = 0.69, time = 0.11 sec\n",
      "Epoch [20] (40/63), train_loss = 0.5524, accuracy = 0.70, time = 0.22 sec\n",
      "Epoch [20] (60/63), train_loss = 0.5295, accuracy = 0.70, time = 0.32 sec\n",
      "Epoch: [20], Test Loss: 0.5886, Accuracy: 0.69, Time: 0.11 sec\n",
      "\n",
      "\n",
      "Epoch [21] (20/63), train_loss = 0.6151, accuracy = 0.69, time = 0.10 sec\n",
      "Epoch [21] (40/63), train_loss = 0.5506, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [21] (60/63), train_loss = 0.5278, accuracy = 0.70, time = 0.32 sec\n",
      "Epoch: [21], Test Loss: 0.5879, Accuracy: 0.69, Time: 0.18 sec\n",
      "\n",
      "\n",
      "Epoch [22] (20/63), train_loss = 0.6147, accuracy = 0.69, time = 0.12 sec\n",
      "Epoch [22] (40/63), train_loss = 0.5489, accuracy = 0.70, time = 0.22 sec\n",
      "Epoch [22] (60/63), train_loss = 0.5263, accuracy = 0.70, time = 0.30 sec\n",
      "Epoch: [22], Test Loss: 0.5872, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [23] (20/63), train_loss = 0.6144, accuracy = 0.69, time = 0.10 sec\n",
      "Epoch [23] (40/63), train_loss = 0.5473, accuracy = 0.70, time = 0.19 sec\n",
      "Epoch [23] (60/63), train_loss = 0.5250, accuracy = 0.70, time = 0.29 sec\n",
      "Epoch: [23], Test Loss: 0.5866, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [24] (20/63), train_loss = 0.6141, accuracy = 0.69, time = 0.09 sec\n",
      "Epoch [24] (40/63), train_loss = 0.5459, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [24] (60/63), train_loss = 0.5237, accuracy = 0.70, time = 0.26 sec\n",
      "Epoch: [24], Test Loss: 0.5861, Accuracy: 0.69, Time: 0.15 sec\n",
      "\n",
      "\n",
      "Epoch [25] (20/63), train_loss = 0.6139, accuracy = 0.69, time = 0.12 sec\n",
      "Epoch [25] (40/63), train_loss = 0.5446, accuracy = 0.70, time = 0.23 sec\n",
      "Epoch [25] (60/63), train_loss = 0.5226, accuracy = 0.70, time = 0.33 sec\n",
      "Epoch: [25], Test Loss: 0.5857, Accuracy: 0.69, Time: 0.11 sec\n",
      "\n",
      "\n",
      "Epoch [26] (20/63), train_loss = 0.6137, accuracy = 0.69, time = 0.09 sec\n",
      "Epoch [26] (40/63), train_loss = 0.5434, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [26] (60/63), train_loss = 0.5216, accuracy = 0.70, time = 0.29 sec\n",
      "Epoch: [26], Test Loss: 0.5853, Accuracy: 0.69, Time: 0.16 sec\n",
      "\n",
      "\n",
      "Epoch [27] (20/63), train_loss = 0.6136, accuracy = 0.68, time = 0.10 sec\n",
      "Epoch [27] (40/63), train_loss = 0.5424, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [27] (60/63), train_loss = 0.5206, accuracy = 0.70, time = 0.28 sec\n",
      "Epoch: [27], Test Loss: 0.5849, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [28] (20/63), train_loss = 0.6135, accuracy = 0.68, time = 0.10 sec\n",
      "Epoch [28] (40/63), train_loss = 0.5414, accuracy = 0.70, time = 0.19 sec\n",
      "Epoch [28] (60/63), train_loss = 0.5197, accuracy = 0.70, time = 0.28 sec\n",
      "Epoch: [28], Test Loss: 0.5846, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [29] (20/63), train_loss = 0.6134, accuracy = 0.68, time = 0.10 sec\n",
      "Epoch [29] (40/63), train_loss = 0.5405, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [29] (60/63), train_loss = 0.5189, accuracy = 0.70, time = 0.27 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29], Test Loss: 0.5843, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [30] (20/63), train_loss = 0.6133, accuracy = 0.69, time = 0.09 sec\n",
      "Epoch [30] (40/63), train_loss = 0.5396, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [30] (60/63), train_loss = 0.5181, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [30], Test Loss: 0.5841, Accuracy: 0.69, Time: 0.11 sec\n",
      "\n",
      "\n",
      "Epoch [31] (20/63), train_loss = 0.6133, accuracy = 0.69, time = 0.09 sec\n",
      "Epoch [31] (40/63), train_loss = 0.5389, accuracy = 0.70, time = 0.19 sec\n",
      "Epoch [31] (60/63), train_loss = 0.5174, accuracy = 0.70, time = 0.28 sec\n",
      "Epoch: [31], Test Loss: 0.5839, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [32] (20/63), train_loss = 0.6132, accuracy = 0.69, time = 0.11 sec\n",
      "Epoch [32] (40/63), train_loss = 0.5381, accuracy = 0.70, time = 0.23 sec\n",
      "Epoch [32] (60/63), train_loss = 0.5168, accuracy = 0.70, time = 0.32 sec\n",
      "Epoch: [32], Test Loss: 0.5837, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [33] (20/63), train_loss = 0.6132, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [33] (40/63), train_loss = 0.5375, accuracy = 0.70, time = 0.19 sec\n",
      "Epoch [33] (60/63), train_loss = 0.5162, accuracy = 0.70, time = 0.30 sec\n",
      "Epoch: [33], Test Loss: 0.5835, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [34] (20/63), train_loss = 0.6132, accuracy = 0.69, time = 0.09 sec\n",
      "Epoch [34] (40/63), train_loss = 0.5369, accuracy = 0.70, time = 0.19 sec\n",
      "Epoch [34] (60/63), train_loss = 0.5156, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [34], Test Loss: 0.5834, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [35] (20/63), train_loss = 0.6132, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [35] (40/63), train_loss = 0.5363, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [35] (60/63), train_loss = 0.5151, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [35], Test Loss: 0.5832, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [36] (20/63), train_loss = 0.6132, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [36] (40/63), train_loss = 0.5358, accuracy = 0.70, time = 0.20 sec\n",
      "Epoch [36] (60/63), train_loss = 0.5146, accuracy = 0.70, time = 0.32 sec\n",
      "Epoch: [36], Test Loss: 0.5831, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [37] (20/63), train_loss = 0.6132, accuracy = 0.69, time = 0.10 sec\n",
      "Epoch [37] (40/63), train_loss = 0.5353, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [37] (60/63), train_loss = 0.5142, accuracy = 0.70, time = 0.30 sec\n",
      "Epoch: [37], Test Loss: 0.5830, Accuracy: 0.69, Time: 0.15 sec\n",
      "\n",
      "\n",
      "Epoch [38] (20/63), train_loss = 0.6132, accuracy = 0.69, time = 0.09 sec\n",
      "Epoch [38] (40/63), train_loss = 0.5348, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [38] (60/63), train_loss = 0.5138, accuracy = 0.70, time = 0.26 sec\n",
      "Epoch: [38], Test Loss: 0.5829, Accuracy: 0.69, Time: 0.11 sec\n",
      "\n",
      "\n",
      "Epoch [39] (20/63), train_loss = 0.6132, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [39] (40/63), train_loss = 0.5344, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [39] (60/63), train_loss = 0.5134, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [39], Test Loss: 0.5828, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [40] (20/63), train_loss = 0.6132, accuracy = 0.69, time = 0.10 sec\n",
      "Epoch [40] (40/63), train_loss = 0.5340, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [40] (60/63), train_loss = 0.5130, accuracy = 0.70, time = 0.28 sec\n",
      "Epoch: [40], Test Loss: 0.5828, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [41] (20/63), train_loss = 0.6133, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [41] (40/63), train_loss = 0.5336, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [41] (60/63), train_loss = 0.5127, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [41], Test Loss: 0.5827, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [42] (20/63), train_loss = 0.6133, accuracy = 0.69, time = 0.10 sec\n",
      "Epoch [42] (40/63), train_loss = 0.5333, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [42] (60/63), train_loss = 0.5123, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [42], Test Loss: 0.5827, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [43] (20/63), train_loss = 0.6133, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [43] (40/63), train_loss = 0.5329, accuracy = 0.70, time = 0.16 sec\n",
      "Epoch [43] (60/63), train_loss = 0.5120, accuracy = 0.70, time = 0.26 sec\n",
      "Epoch: [43], Test Loss: 0.5826, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [44] (20/63), train_loss = 0.6133, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [44] (40/63), train_loss = 0.5326, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [44] (60/63), train_loss = 0.5118, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [44], Test Loss: 0.5826, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [45] (20/63), train_loss = 0.6134, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [45] (40/63), train_loss = 0.5324, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [45] (60/63), train_loss = 0.5115, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [45], Test Loss: 0.5825, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [46] (20/63), train_loss = 0.6134, accuracy = 0.69, time = 0.12 sec\n",
      "Epoch [46] (40/63), train_loss = 0.5321, accuracy = 0.70, time = 0.20 sec\n",
      "Epoch [46] (60/63), train_loss = 0.5113, accuracy = 0.70, time = 0.28 sec\n",
      "Epoch: [46], Test Loss: 0.5825, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [47] (20/63), train_loss = 0.6134, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [47] (40/63), train_loss = 0.5318, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [47] (60/63), train_loss = 0.5110, accuracy = 0.70, time = 0.25 sec\n",
      "Epoch: [47], Test Loss: 0.5825, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [48] (20/63), train_loss = 0.6135, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [48] (40/63), train_loss = 0.5316, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [48] (60/63), train_loss = 0.5108, accuracy = 0.70, time = 0.26 sec\n",
      "Epoch: [48], Test Loss: 0.5824, Accuracy: 0.69, Time: 0.16 sec\n",
      "\n",
      "\n",
      "Epoch [49] (20/63), train_loss = 0.6135, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [49] (40/63), train_loss = 0.5314, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [49] (60/63), train_loss = 0.5106, accuracy = 0.70, time = 0.25 sec\n",
      "Epoch: [49], Test Loss: 0.5824, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [50] (20/63), train_loss = 0.6135, accuracy = 0.69, time = 0.09 sec\n",
      "Epoch [50] (40/63), train_loss = 0.5312, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [50] (60/63), train_loss = 0.5104, accuracy = 0.70, time = 0.25 sec\n",
      "Epoch: [50], Test Loss: 0.5824, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [51] (20/63), train_loss = 0.6136, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [51] (40/63), train_loss = 0.5310, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [51] (60/63), train_loss = 0.5102, accuracy = 0.70, time = 0.28 sec\n",
      "Epoch: [51], Test Loss: 0.5824, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [52] (20/63), train_loss = 0.6136, accuracy = 0.69, time = 0.09 sec\n",
      "Epoch [52] (40/63), train_loss = 0.5308, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [52] (60/63), train_loss = 0.5100, accuracy = 0.70, time = 0.25 sec\n",
      "Epoch: [52], Test Loss: 0.5824, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [53] (20/63), train_loss = 0.6136, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [53] (40/63), train_loss = 0.5306, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [53] (60/63), train_loss = 0.5099, accuracy = 0.70, time = 0.25 sec\n",
      "Epoch: [53], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [54] (20/63), train_loss = 0.6137, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [54] (40/63), train_loss = 0.5305, accuracy = 0.70, time = 0.19 sec\n",
      "Epoch [54] (60/63), train_loss = 0.5097, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [54], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.11 sec\n",
      "\n",
      "\n",
      "Epoch [55] (20/63), train_loss = 0.6137, accuracy = 0.69, time = 0.09 sec\n",
      "Epoch [55] (40/63), train_loss = 0.5303, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [55] (60/63), train_loss = 0.5096, accuracy = 0.70, time = 0.26 sec\n",
      "Epoch: [55], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [56] (20/63), train_loss = 0.6137, accuracy = 0.69, time = 0.11 sec\n",
      "Epoch [56] (40/63), train_loss = 0.5301, accuracy = 0.70, time = 0.20 sec\n",
      "Epoch [56] (60/63), train_loss = 0.5095, accuracy = 0.70, time = 0.28 sec\n",
      "Epoch: [56], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.09 sec\n",
      "\n",
      "\n",
      "Epoch [57] (20/63), train_loss = 0.6138, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [57] (40/63), train_loss = 0.5300, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [57] (60/63), train_loss = 0.5093, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [57], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [58] (20/63), train_loss = 0.6138, accuracy = 0.69, time = 0.11 sec\n",
      "Epoch [58] (40/63), train_loss = 0.5299, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [58] (60/63), train_loss = 0.5092, accuracy = 0.70, time = 0.29 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [58], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.11 sec\n",
      "\n",
      "\n",
      "Epoch [59] (20/63), train_loss = 0.6138, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [59] (40/63), train_loss = 0.5298, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [59] (60/63), train_loss = 0.5091, accuracy = 0.70, time = 0.25 sec\n",
      "Epoch: [59], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.13 sec\n",
      "\n",
      "\n",
      "Epoch [60] (20/63), train_loss = 0.6138, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [60] (40/63), train_loss = 0.5296, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [60] (60/63), train_loss = 0.5090, accuracy = 0.70, time = 0.29 sec\n",
      "Epoch: [60], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [61] (20/63), train_loss = 0.6139, accuracy = 0.69, time = 0.09 sec\n",
      "Epoch [61] (40/63), train_loss = 0.5295, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [61] (60/63), train_loss = 0.5089, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [61], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [62] (20/63), train_loss = 0.6139, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [62] (40/63), train_loss = 0.5294, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [62] (60/63), train_loss = 0.5088, accuracy = 0.70, time = 0.25 sec\n",
      "Epoch: [62], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [63] (20/63), train_loss = 0.6139, accuracy = 0.69, time = 0.10 sec\n",
      "Epoch [63] (40/63), train_loss = 0.5293, accuracy = 0.70, time = 0.22 sec\n",
      "Epoch [63] (60/63), train_loss = 0.5087, accuracy = 0.70, time = 0.32 sec\n",
      "Epoch: [63], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [64] (20/63), train_loss = 0.6139, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [64] (40/63), train_loss = 0.5292, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [64] (60/63), train_loss = 0.5086, accuracy = 0.70, time = 0.25 sec\n",
      "Epoch: [64], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [65] (20/63), train_loss = 0.6140, accuracy = 0.69, time = 0.10 sec\n",
      "Epoch [65] (40/63), train_loss = 0.5292, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [65] (60/63), train_loss = 0.5085, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [65], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.11 sec\n",
      "\n",
      "\n",
      "Epoch [66] (20/63), train_loss = 0.6140, accuracy = 0.69, time = 0.10 sec\n",
      "Epoch [66] (40/63), train_loss = 0.5291, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [66] (60/63), train_loss = 0.5084, accuracy = 0.70, time = 0.28 sec\n",
      "Epoch: [66], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [67] (20/63), train_loss = 0.6140, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [67] (40/63), train_loss = 0.5290, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [67] (60/63), train_loss = 0.5084, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [67], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [68] (20/63), train_loss = 0.6140, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [68] (40/63), train_loss = 0.5289, accuracy = 0.70, time = 0.16 sec\n",
      "Epoch [68] (60/63), train_loss = 0.5083, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [68], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.11 sec\n",
      "\n",
      "\n",
      "Epoch [69] (20/63), train_loss = 0.6140, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [69] (40/63), train_loss = 0.5288, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [69] (60/63), train_loss = 0.5082, accuracy = 0.70, time = 0.25 sec\n",
      "Epoch: [69], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [70] (20/63), train_loss = 0.6141, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [70] (40/63), train_loss = 0.5288, accuracy = 0.70, time = 0.16 sec\n",
      "Epoch [70] (60/63), train_loss = 0.5082, accuracy = 0.70, time = 0.26 sec\n",
      "Epoch: [70], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [71] (20/63), train_loss = 0.6141, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [71] (40/63), train_loss = 0.5287, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [71] (60/63), train_loss = 0.5081, accuracy = 0.70, time = 0.25 sec\n",
      "Epoch: [71], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [72] (20/63), train_loss = 0.6141, accuracy = 0.69, time = 0.09 sec\n",
      "Epoch [72] (40/63), train_loss = 0.5287, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [72] (60/63), train_loss = 0.5081, accuracy = 0.70, time = 0.25 sec\n",
      "Epoch: [72], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [73] (20/63), train_loss = 0.6141, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [73] (40/63), train_loss = 0.5286, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [73] (60/63), train_loss = 0.5080, accuracy = 0.70, time = 0.25 sec\n",
      "Epoch: [73], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [74] (20/63), train_loss = 0.6141, accuracy = 0.69, time = 0.09 sec\n",
      "Epoch [74] (40/63), train_loss = 0.5285, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [74] (60/63), train_loss = 0.5079, accuracy = 0.70, time = 0.25 sec\n",
      "Epoch: [74], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [75] (20/63), train_loss = 0.6141, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [75] (40/63), train_loss = 0.5285, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [75] (60/63), train_loss = 0.5079, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [75], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [76] (20/63), train_loss = 0.6142, accuracy = 0.69, time = 0.10 sec\n",
      "Epoch [76] (40/63), train_loss = 0.5284, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [76] (60/63), train_loss = 0.5079, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [76], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [77] (20/63), train_loss = 0.6142, accuracy = 0.69, time = 0.10 sec\n",
      "Epoch [77] (40/63), train_loss = 0.5284, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [77] (60/63), train_loss = 0.5078, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [77], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [78] (20/63), train_loss = 0.6142, accuracy = 0.69, time = 0.10 sec\n",
      "Epoch [78] (40/63), train_loss = 0.5283, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [78] (60/63), train_loss = 0.5078, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [78], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [79] (20/63), train_loss = 0.6142, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [79] (40/63), train_loss = 0.5283, accuracy = 0.70, time = 0.16 sec\n",
      "Epoch [79] (60/63), train_loss = 0.5077, accuracy = 0.70, time = 0.26 sec\n",
      "Epoch: [79], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [80] (20/63), train_loss = 0.6142, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [80] (40/63), train_loss = 0.5283, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [80] (60/63), train_loss = 0.5077, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [80], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [81] (20/63), train_loss = 0.6142, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [81] (40/63), train_loss = 0.5282, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [81] (60/63), train_loss = 0.5077, accuracy = 0.70, time = 0.25 sec\n",
      "Epoch: [81], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [82] (20/63), train_loss = 0.6142, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [82] (40/63), train_loss = 0.5282, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [82] (60/63), train_loss = 0.5076, accuracy = 0.70, time = 0.26 sec\n",
      "Epoch: [82], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [83] (20/63), train_loss = 0.6142, accuracy = 0.69, time = 0.10 sec\n",
      "Epoch [83] (40/63), train_loss = 0.5282, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [83] (60/63), train_loss = 0.5076, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [83], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [84] (20/63), train_loss = 0.6143, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [84] (40/63), train_loss = 0.5281, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [84] (60/63), train_loss = 0.5076, accuracy = 0.70, time = 0.26 sec\n",
      "Epoch: [84], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [85] (20/63), train_loss = 0.6143, accuracy = 0.69, time = 0.09 sec\n",
      "Epoch [85] (40/63), train_loss = 0.5281, accuracy = 0.70, time = 0.19 sec\n",
      "Epoch [85] (60/63), train_loss = 0.5075, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [85], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [86] (20/63), train_loss = 0.6143, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [86] (40/63), train_loss = 0.5281, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [86] (60/63), train_loss = 0.5075, accuracy = 0.70, time = 0.25 sec\n",
      "Epoch: [86], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [87] (20/63), train_loss = 0.6143, accuracy = 0.69, time = 0.09 sec\n",
      "Epoch [87] (40/63), train_loss = 0.5280, accuracy = 0.70, time = 0.18 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [87] (60/63), train_loss = 0.5075, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [87], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [88] (20/63), train_loss = 0.6143, accuracy = 0.69, time = 0.10 sec\n",
      "Epoch [88] (40/63), train_loss = 0.5280, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [88] (60/63), train_loss = 0.5075, accuracy = 0.70, time = 0.27 sec\n",
      "Epoch: [88], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.12 sec\n",
      "\n",
      "\n",
      "Epoch [89] (20/63), train_loss = 0.6143, accuracy = 0.69, time = 0.08 sec\n",
      "Epoch [89] (40/63), train_loss = 0.5280, accuracy = 0.70, time = 0.17 sec\n",
      "Epoch [89] (60/63), train_loss = 0.5074, accuracy = 0.70, time = 0.26 sec\n",
      "Epoch: [89], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [90] (20/63), train_loss = 0.6143, accuracy = 0.69, time = 0.07 sec\n",
      "Epoch [90] (40/63), train_loss = 0.5280, accuracy = 0.70, time = 0.16 sec\n",
      "Epoch [90] (60/63), train_loss = 0.5074, accuracy = 0.70, time = 0.24 sec\n",
      "Epoch: [90], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.10 sec\n",
      "\n",
      "\n",
      "Epoch [91] (20/63), train_loss = 0.6143, accuracy = 0.69, time = 0.10 sec\n",
      "Epoch [91] (40/63), train_loss = 0.5279, accuracy = 0.70, time = 0.18 sec\n",
      "Epoch [91] (60/63), train_loss = 0.5074, accuracy = 0.70, time = 0.30 sec\n",
      "Epoch: [91], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.14 sec\n",
      "\n",
      "\n",
      "Epoch [92] (20/63), train_loss = 0.6143, accuracy = 0.69, time = 0.12 sec\n",
      "Epoch [92] (40/63), train_loss = 0.5279, accuracy = 0.70, time = 0.24 sec\n",
      "Epoch [92] (60/63), train_loss = 0.5074, accuracy = 0.70, time = 0.36 sec\n",
      "Epoch: [92], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.13 sec\n",
      "\n",
      "\n",
      "Epoch [93] (20/63), train_loss = 0.6143, accuracy = 0.69, time = 0.12 sec\n",
      "Epoch [93] (40/63), train_loss = 0.5279, accuracy = 0.70, time = 0.24 sec\n",
      "Epoch [93] (60/63), train_loss = 0.5073, accuracy = 0.70, time = 0.34 sec\n",
      "Epoch: [93], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.14 sec\n",
      "\n",
      "\n",
      "Epoch [94] (20/63), train_loss = 0.6143, accuracy = 0.69, time = 0.12 sec\n",
      "Epoch [94] (40/63), train_loss = 0.5279, accuracy = 0.70, time = 0.25 sec\n",
      "Epoch [94] (60/63), train_loss = 0.5073, accuracy = 0.70, time = 0.37 sec\n",
      "Epoch: [94], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.13 sec\n",
      "\n",
      "\n",
      "Epoch [95] (20/63), train_loss = 0.6143, accuracy = 0.69, time = 0.12 sec\n",
      "Epoch [95] (40/63), train_loss = 0.5279, accuracy = 0.70, time = 0.24 sec\n",
      "Epoch [95] (60/63), train_loss = 0.5073, accuracy = 0.70, time = 0.35 sec\n",
      "Epoch: [95], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.13 sec\n",
      "\n",
      "\n",
      "Epoch [96] (20/63), train_loss = 0.6143, accuracy = 0.69, time = 0.12 sec\n",
      "Epoch [96] (40/63), train_loss = 0.5278, accuracy = 0.70, time = 0.24 sec\n",
      "Epoch [96] (60/63), train_loss = 0.5073, accuracy = 0.70, time = 0.36 sec\n",
      "Epoch: [96], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.14 sec\n",
      "\n",
      "\n",
      "Epoch [97] (20/63), train_loss = 0.6144, accuracy = 0.69, time = 0.10 sec\n",
      "Epoch [97] (40/63), train_loss = 0.5278, accuracy = 0.70, time = 0.22 sec\n",
      "Epoch [97] (60/63), train_loss = 0.5073, accuracy = 0.70, time = 0.33 sec\n",
      "Epoch: [97], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.14 sec\n",
      "\n",
      "\n",
      "Epoch [98] (20/63), train_loss = 0.6144, accuracy = 0.69, time = 0.12 sec\n",
      "Epoch [98] (40/63), train_loss = 0.5278, accuracy = 0.70, time = 0.24 sec\n",
      "Epoch [98] (60/63), train_loss = 0.5073, accuracy = 0.70, time = 0.37 sec\n",
      "Epoch: [98], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.14 sec\n",
      "\n",
      "\n",
      "Epoch [99] (20/63), train_loss = 0.6144, accuracy = 0.69, time = 0.12 sec\n",
      "Epoch [99] (40/63), train_loss = 0.5278, accuracy = 0.70, time = 0.25 sec\n",
      "Epoch [99] (60/63), train_loss = 0.5072, accuracy = 0.70, time = 0.41 sec\n",
      "Epoch: [99], Test Loss: 0.5823, Accuracy: 0.69, Time: 0.14 sec\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "for epoch in range(1, 100):\n",
    "  train_loss.append(train(epoch))\n",
    "  test_loss.append(test(epoch))\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d6073a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the test vs train loss curve\n",
    "plt.plot(range(1, len(train_loss)+1), train_loss, label = \"Training loss\")\n",
    "plt.plot(range(1, len(test_loss)+1), test_loss, label = \"Test loss\")\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Training vs Test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca446801",
   "metadata": {},
   "source": [
    "### *Model is underfitting, so it can be improved by making the model more complex*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b2c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7f39c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
