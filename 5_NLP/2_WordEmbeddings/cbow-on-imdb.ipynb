{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-19T09:31:10.777406Z",
     "iopub.status.busy": "2023-01-19T09:31:10.776773Z",
     "iopub.status.idle": "2023-01-19T09:31:25.198076Z",
     "shell.execute_reply": "2023-01-19T09:31:25.196801Z",
     "shell.execute_reply.started": "2023-01-19T09:31:10.777300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-01-19T09:31:27.953007Z",
     "iopub.status.busy": "2023-01-19T09:31:27.952451Z",
     "iopub.status.idle": "2023-01-19T09:31:32.380849Z",
     "shell.execute_reply": "2023-01-19T09:31:32.379845Z",
     "shell.execute_reply.started": "2023-01-19T09:31:27.952959Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from time import time \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-19T09:31:32.385011Z",
     "iopub.status.busy": "2023-01-19T09:31:32.384520Z",
     "iopub.status.idle": "2023-01-19T09:31:32.503634Z",
     "shell.execute_reply": "2023-01-19T09:31:32.502604Z",
     "shell.execute_reply.started": "2023-01-19T09:31:32.384983Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "punct = string.punctuation\n",
    "stopwords_english = stopwords.words('english')\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-19T09:31:36.126379Z",
     "iopub.status.busy": "2023-01-19T09:31:36.126009Z",
     "iopub.status.idle": "2023-01-19T09:31:36.596855Z",
     "shell.execute_reply": "2023-01-19T09:31:36.595946Z",
     "shell.execute_reply.started": "2023-01-19T09:31:36.126347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first think another Disney movie, might good, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big fan Stephen King's work, film made even gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truly enjoyed film. acting terrific plot. Jeff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>memory \"The Last Hunt\" stuck since saw 1956 13...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shakespeare fan, appreciate Ken Branagh done b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>privilege watching Scarface big screen beautif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>real classic. shipload sailors trying get town...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Serials short subjects originally shown theate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  polarity\n",
       "0  first think another Disney movie, might good, ...         1\n",
       "1  Put aside Dr. House repeat missed, Desperate H...         0\n",
       "2  big fan Stephen King's work, film made even gr...         1\n",
       "3  watched horrid thing TV. Needless say one movi...         0\n",
       "4  truly enjoyed film. acting terrific plot. Jeff...         1\n",
       "5  memory \"The Last Hunt\" stuck since saw 1956 13...         1\n",
       "6  Shakespeare fan, appreciate Ken Branagh done b...         0\n",
       "7  privilege watching Scarface big screen beautif...         1\n",
       "8  real classic. shipload sailors trying get town...         1\n",
       "9  Serials short subjects originally shown theate...         1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/kaggle/input/imdb-movie-reviews/imdb.csv\")     \n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-19T09:31:40.210932Z",
     "iopub.status.busy": "2023-01-19T09:31:40.210554Z",
     "iopub.status.idle": "2023-01-19T09:31:40.227467Z",
     "shell.execute_reply": "2023-01-19T09:31:40.226466Z",
     "shell.execute_reply.started": "2023-01-19T09:31:40.210902Z"
    }
   },
   "outputs": [],
   "source": [
    "negative_data = data[data['polarity']==0]\n",
    "positive_data = data[data['polarity']==1]\n",
    "dataset = pd.concat([negative_data[:50], positive_data[:50]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T14:52:12.056451Z",
     "iopub.status.busy": "2023-01-18T14:52:12.056083Z",
     "iopub.status.idle": "2023-01-18T14:52:12.070504Z",
     "shell.execute_reply": "2023-01-18T14:52:12.069404Z",
     "shell.execute_reply.started": "2023-01-18T14:52:12.056414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shakespeare fan, appreciate Ken Branagh done b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>strange sex comedy there`s little comedy whole...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>many problems film, worst continuity; re-editi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Summer Phoenix great performance really feel s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Domini Enfilren (Marlene Dietrich) spent life ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>movie all, action, fighting, dancing, bull rid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>enjoyed watching Cliffhanger, beginning woman ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Eskimo serious movie cultural chasm indigenous...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  polarity\n",
       "1   Put aside Dr. House repeat missed, Desperate H...         0\n",
       "3   watched horrid thing TV. Needless say one movi...         0\n",
       "6   Shakespeare fan, appreciate Ken Branagh done b...         0\n",
       "10  strange sex comedy there`s little comedy whole...         0\n",
       "11  many problems film, worst continuity; re-editi...         0\n",
       "..                                                ...       ...\n",
       "81  Summer Phoenix great performance really feel s...         1\n",
       "83  Domini Enfilren (Marlene Dietrich) spent life ...         1\n",
       "84  movie all, action, fighting, dancing, bull rid...         1\n",
       "85  enjoyed watching Cliffhanger, beginning woman ...         1\n",
       "87  Eskimo serious movie cultural chasm indigenous...         1\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I will perform the task only on 50 positive and 50 negative tweets, so that my notebook doesn't run out of memory allocated to the sparse vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT PREPOCESSSING AND VOCAB BUILDING\n",
    "(I am ignoring for the specific requirements of preprocessing for this task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T14:52:12.072670Z",
     "iopub.status.busy": "2023-01-18T14:52:12.072207Z",
     "iopub.status.idle": "2023-01-18T14:52:12.443309Z",
     "shell.execute_reply": "2023-01-18T14:52:12.442182Z",
     "shell.execute_reply.started": "2023-01-18T14:52:12.072632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
       "      <td>0</td>\n",
       "      <td>[put, asid, dr, hous, repeat, miss, desper, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[watch, horrid, thing, tv, needless, say, one,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shakespeare fan, appreciate Ken Branagh done b...</td>\n",
       "      <td>0</td>\n",
       "      <td>[shakespear, fan, appreci, ken, branagh, done,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>strange sex comedy there`s little comedy whole...</td>\n",
       "      <td>0</td>\n",
       "      <td>[strang, sex, comedi, there`, littl, comedi, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>many problems film, worst continuity; re-editi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[mani, problem, film, worst, continuity;, re-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Rosie wasted lot TV time talking Tainos super ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[rosi, wast, lot, tv, time, talk, taino, super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>awful, awful! old room mate used watch junk dr...</td>\n",
       "      <td>0</td>\n",
       "      <td>[aw, awful!, old, room, mate, use, watch, junk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Filmfour going lot better little snot film the...</td>\n",
       "      <td>0</td>\n",
       "      <td>[filmfour, go, lot, better, littl, snot, film,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>60s (1999) D: Mark Piznarski. Josh Hamilton, J...</td>\n",
       "      <td>0</td>\n",
       "      <td>[60, (1999), d:, mark, piznarski, josh, hamilt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>show suck? Unfortunately, really question, dou...</td>\n",
       "      <td>0</td>\n",
       "      <td>[show, suck?, unfortun, realli, question, doub...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  polarity  \\\n",
       "1   Put aside Dr. House repeat missed, Desperate H...         0   \n",
       "3   watched horrid thing TV. Needless say one movi...         0   \n",
       "6   Shakespeare fan, appreciate Ken Branagh done b...         0   \n",
       "10  strange sex comedy there`s little comedy whole...         0   \n",
       "11  many problems film, worst continuity; re-editi...         0   \n",
       "12  Rosie wasted lot TV time talking Tainos super ...         0   \n",
       "20  awful, awful! old room mate used watch junk dr...         0   \n",
       "24  Filmfour going lot better little snot film the...         0   \n",
       "25  60s (1999) D: Mark Piznarski. Josh Hamilton, J...         0   \n",
       "26  show suck? Unfortunately, really question, dou...         0   \n",
       "\n",
       "                                         preprocessed  \n",
       "1   [put, asid, dr, hous, repeat, miss, desper, ho...  \n",
       "3   [watch, horrid, thing, tv, needless, say, one,...  \n",
       "6   [shakespear, fan, appreci, ken, branagh, done,...  \n",
       "10  [strang, sex, comedi, there`, littl, comedi, w...  \n",
       "11  [mani, problem, film, worst, continuity;, re-e...  \n",
       "12  [rosi, wast, lot, tv, time, talk, taino, super...  \n",
       "20  [aw, awful!, old, room, mate, use, watch, junk...  \n",
       "24  [filmfour, go, lot, better, littl, snot, film,...  \n",
       "25  [60, (1999), d:, mark, piznarski, josh, hamilt...  \n",
       "26  [show, suck?, unfortun, realli, question, doub...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessing(string, stopwords, stemmer):\n",
    "    '''We can do all the preprocessing in just one step by creating a pipeline\n",
    "    First, we have to make all the words in lowercase,\n",
    "    then we have to tokenize the string,\n",
    "    then we have to remove stopwords and \n",
    "    finally we have to stem all the words.\n",
    "    This is how it will be ready to be analyzed further'''\n",
    "    string  = string.lower()\n",
    "    tokens = re.split('\\s|(?<!\\d)[,.](?!\\d)', string)\n",
    "    clean_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stopwords:\n",
    "            clean_tokens.append(word)\n",
    "    \n",
    "    stemmed_words = []\n",
    "    for word in clean_tokens:\n",
    "        stemmed_words.append(stemmer.stem(word))\n",
    "    \n",
    "    preprocessed_array = []\n",
    "    for word in stemmed_words:\n",
    "        if word!='':\n",
    "            preprocessed_array.append(word)\n",
    "            \n",
    "    return preprocessed_array\n",
    "#########################################\n",
    "dataset['preprocessed'] = dataset['text'].apply(lambda x:preprocessing(x, stopwords_english, stemmer))    \n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T14:52:12.445619Z",
     "iopub.status.busy": "2023-01-18T14:52:12.444935Z",
     "iopub.status.idle": "2023-01-18T14:52:12.457408Z",
     "shell.execute_reply": "2023-01-18T14:52:12.456282Z",
     "shell.execute_reply.started": "2023-01-18T14:52:12.445580Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_frequency(processed_strings, polarities):\n",
    "    vocab = {}\n",
    "    for string, polarity in zip(processed_strings, polarities):\n",
    "        for word in string:\n",
    "            pair = (word, polarity)\n",
    "            if pair in vocab:\n",
    "                vocab[pair]+=1\n",
    "            else:\n",
    "                vocab[pair] = 1\n",
    "    return vocab\n",
    "#####################################\n",
    "vocab = build_frequency(dataset['preprocessed'], dataset['polarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First we have to figure out word2indices and indices2word dictionary for one hot encodings and form the final data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T14:52:12.459744Z",
     "iopub.status.busy": "2023-01-18T14:52:12.459360Z",
     "iopub.status.idle": "2023-01-18T14:52:12.475582Z",
     "shell.execute_reply": "2023-01-18T14:52:12.474565Z",
     "shell.execute_reply.started": "2023-01-18T14:52:12.459707Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_unique_words(vocab):\n",
    "    unique_words = set()\n",
    "    for key in vocab.keys():\n",
    "        unique_words.add(key[0])\n",
    "    return unique_words\n",
    "\n",
    "\n",
    "unique_words = find_unique_words(vocab)\n",
    "###################################\n",
    "def word2ind_ind2word(unique_words):\n",
    "    # \n",
    "    word2ind = {}\n",
    "    ind2word  ={}\n",
    "    for index, word in enumerate(unique_words):\n",
    "        word2ind[word] = index\n",
    "        ind2word[index] = word\n",
    "    return word2ind, ind2word\n",
    "\n",
    "word2ind, ind2word = word2ind_ind2word(unique_words)\n",
    "########################################    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T14:52:12.480125Z",
     "iopub.status.busy": "2023-01-18T14:52:12.479750Z",
     "iopub.status.idle": "2023-01-18T14:52:12.488580Z",
     "shell.execute_reply": "2023-01-18T14:52:12.487373Z",
     "shell.execute_reply.started": "2023-01-18T14:52:12.480094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4078, 4078, 4078)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_words), len(word2ind), len(ind2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we have to form the data, first we have to form the data for a single string with the help of sliding windows and word2index dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T14:52:12.490592Z",
     "iopub.status.busy": "2023-01-18T14:52:12.489959Z",
     "iopub.status.idle": "2023-01-18T14:52:12.502953Z",
     "shell.execute_reply": "2023-01-18T14:52:12.501897Z",
     "shell.execute_reply.started": "2023-01-18T14:52:12.490554Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "C=2\n",
    "intermediate_length = 300\n",
    "one_hot_length = len(unique_words)\n",
    "\n",
    "def data_for_one_processed_tweet(processed_tweet, word2ind=word2ind, in2word=ind2word, C=2, one_hot_length=one_hot_length):    \n",
    "    # string will be a one token list\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    for i in range(C, len(processed_tweet)-C):\n",
    "        x = np.zeros(one_hot_length, dtype=np.float32)\n",
    "        temp_list = processed_tweet[i-C:i] + processed_tweet[i+1:i+C+1]\n",
    "        for word in temp_list:\n",
    "           x[word2ind[word]]+= 1\n",
    "        x/=(2*C)\n",
    "        data_x.append(x)\n",
    "        data_y.append(word2ind[processed_tweet[i]])\n",
    "    return data_x, data_y\n",
    "\n",
    "######################################\n",
    "def full_data(processed_tweets):\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    for processed_tweet in processed_tweets:\n",
    "        x,y = data_for_one_processed_tweet(processed_tweet)\n",
    "        data_x+= x\n",
    "        data_y+= y\n",
    "    data_x = np.array(data_x, dtype=np.float32)\n",
    "    data_y = np.array(data_y, dtype=np.int32)\n",
    "    data_y = data_y.reshape(data_y.shape[0], 1)\n",
    "    return np.concatenate((data_x, data_y), axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model for CBOW**\n",
    "* Now for capturing the context of the words we will apply the cbow model where c=2 and take the intermediate length of the vectors to be 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T14:52:12.505219Z",
     "iopub.status.busy": "2023-01-18T14:52:12.504758Z",
     "iopub.status.idle": "2023-01-18T14:52:12.516841Z",
     "shell.execute_reply": "2023-01-18T14:52:12.516055Z",
     "shell.execute_reply.started": "2023-01-18T14:52:12.505179Z"
    }
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, length, intermediate_len):\n",
    "        super(CBOW, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_features=length, out_features=intermediate_len))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(in_features=intermediate_len, out_features=length))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T14:52:12.519099Z",
     "iopub.status.busy": "2023-01-18T14:52:12.518231Z",
     "iopub.status.idle": "2023-01-18T14:52:17.309917Z",
     "shell.execute_reply": "2023-01-18T14:52:17.308643Z",
     "shell.execute_reply.started": "2023-01-18T14:52:12.519061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 300]       1,223,700\n",
      "              ReLU-2                  [-1, 300]               0\n",
      "            Linear-3                 [-1, 4078]       1,227,478\n",
      "================================================================\n",
      "Total params: 2,451,178\n",
      "Trainable params: 2,451,178\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 9.35\n",
      "Estimated Total Size (MB): 9.40\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "cbow = CBOW(length=len(unique_words), intermediate_len=300).to(device)\n",
    "summary(cbow, (len(unique_words), ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **UTILS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T14:52:17.312880Z",
     "iopub.status.busy": "2023-01-18T14:52:17.311732Z",
     "iopub.status.idle": "2023-01-18T14:52:17.318514Z",
     "shell.execute_reply": "2023-01-18T14:52:17.317368Z",
     "shell.execute_reply.started": "2023-01-18T14:52:17.312815Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename='my_checkpoint.pth.tar'):\n",
    "    # will save model and optimizer params at every epoch\n",
    "    print(\"-> Saving CheckPoint\")\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T14:52:17.321121Z",
     "iopub.status.busy": "2023-01-18T14:52:17.320008Z",
     "iopub.status.idle": "2023-01-18T14:52:17.331692Z",
     "shell.execute_reply": "2023-01-18T14:52:17.330624Z",
     "shell.execute_reply.started": "2023-01-18T14:52:17.321078Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint, model):\n",
    "    # it will just load, we can train it further, make changes to the architecture\n",
    "    # and simply use it to predict\n",
    "    print(\"-> Loading CheckPoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T14:52:17.333839Z",
     "iopub.status.busy": "2023-01-18T14:52:17.333184Z",
     "iopub.status.idle": "2023-01-18T14:52:17.347663Z",
     "shell.execute_reply": "2023-01-18T14:52:17.346301Z",
     "shell.execute_reply.started": "2023-01-18T14:52:17.333795Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(loader, model, optimizer, loss_fn, scaler, device=device):\n",
    "    '''\n",
    "    it is the training procedure for one epoch of the network\n",
    "    '''\n",
    "    num_batches = len(loader)\n",
    "    batches = tqdm(loader) # tqdm will be used to generate progress bars\n",
    "    acc = 0\n",
    "    for idx, batch in enumerate(batches, 0):\n",
    "        inp = batch[:, :-1].type(torch.float32).to(device)  # shape is (32, len(unique_words))\n",
    "        target = batch[:, -1].type(torch.LongTensor).to(device) # shape is (32,)\n",
    "        \n",
    "        # forward\n",
    "        #with torch.cuda.amp.autocast(): # for gradient underflowing and overflowing and it makes training faster by converting all floats to float16\n",
    "        pred = model(inp)\n",
    "        loss = loss_fn(pred, target) \n",
    "            \n",
    "        optimizer.zero_grad()  # making all the previous gradients zero \n",
    "        #scaler.scale(loss).backward()\n",
    "        loss.backward()\n",
    "        #scaler.step(optimizer)\n",
    "        #scaler.update()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            acc += (torch.argmax(pred, dim=1)==target).sum().item()/batch_size\n",
    "        batches.set_postfix(loss = loss.item()) # loss of this current batch on current iteration \n",
    "        model.train()\n",
    "\n",
    "    acc/=num_batches\n",
    "    print(f\"Validation accuracy is {acc*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DRIVER CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T14:52:17.351787Z",
     "iopub.status.busy": "2023-01-18T14:52:17.351442Z",
     "iopub.status.idle": "2023-01-18T14:52:18.002575Z",
     "shell.execute_reply": "2023-01-18T14:52:18.001569Z",
     "shell.execute_reply.started": "2023-01-18T14:52:17.351727Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 32\n",
    "num_workers = 2\n",
    "num_epochs = 20 # just for testing if loop works fine else i am gonna set it to 100\n",
    "\n",
    "# set up the input data\n",
    "train_data = full_data(dataset['preprocessed']) \n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True,\n",
    "                                           num_workers=num_workers)\n",
    "\n",
    "\n",
    "#setting up the model, scaler, optimizer, loss_fn\n",
    "model = CBOW(length=len(unique_words), intermediate_len=300).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T14:52:18.004733Z",
     "iopub.status.busy": "2023-01-18T14:52:18.004331Z",
     "iopub.status.idle": "2023-01-18T14:54:39.552535Z",
     "shell.execute_reply": "2023-01-18T14:54:39.551194Z",
     "shell.execute_reply.started": "2023-01-18T14:52:18.004690Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 132.19it/s, loss=7.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 2.0\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 121.53it/s, loss=7.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 3.9553571428571432\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 136.26it/s, loss=7.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 4.803571428571429\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 129.61it/s, loss=7.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 4.866071428571429\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 117.23it/s, loss=7.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 4.883928571428572\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 119.40it/s, loss=6.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 5.348214285714286\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 128.00it/s, loss=5.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 5.866071428571429\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 135.72it/s, loss=5.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 6.964285714285714\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 125.39it/s, loss=5.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 9.883928571428571\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 131.48it/s, loss=4.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 16.178571428571427\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 128.16it/s, loss=4.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 25.455357142857142\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 135.92it/s, loss=3.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 35.732142857142854\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 131.61it/s, loss=2.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 49.6875\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 123.45it/s, loss=1.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 65.17857142857143\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 136.42it/s, loss=1.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 80.15178571428572\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 132.20it/s, loss=0.89] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 88.16071428571428\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:03<00:00, 105.27it/s, loss=0.526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 92.91964285714286\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 128.84it/s, loss=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 95.54464285714286\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 135.81it/s, loss=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 96.95535714285714\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 136.44it/s, loss=0.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 97.97321428571428\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 120.50it/s, loss=0.32] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 98.625\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 128.62it/s, loss=0.138] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.00892857142857\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 130.42it/s, loss=0.175] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.21428571428571\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 134.26it/s, loss=0.127] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.42857142857143\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 118.83it/s, loss=0.0912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.50892857142857\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 136.40it/s, loss=0.0456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.61607142857143\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 134.86it/s, loss=0.0298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.61607142857143\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:03<00:00, 110.30it/s, loss=0.0289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.63392857142857\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 124.63it/s, loss=0.0362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.6875\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 133.73it/s, loss=0.0261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.72321428571429\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 136.31it/s, loss=0.0277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.70535714285714\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 126.74it/s, loss=0.0186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.75\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 123.73it/s, loss=0.0241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.73214285714286\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 135.14it/s, loss=0.0149] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.73214285714286\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 135.54it/s, loss=0.0171] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.73214285714286\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 129.81it/s, loss=0.00922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.74107142857143\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 119.97it/s, loss=0.0175] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.75892857142857\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 133.96it/s, loss=0.00674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.72321428571429\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:03<00:00, 109.15it/s, loss=0.00707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.74107142857143\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 132.21it/s, loss=0.00574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.75\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 128.39it/s, loss=0.0079] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.74107142857143\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 133.69it/s, loss=0.00312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.73214285714286\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 130.78it/s, loss=0.00739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.75892857142857\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 134.93it/s, loss=0.00917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.71428571428571\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 125.88it/s, loss=0.00234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.73214285714286\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 127.94it/s, loss=0.000932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.75\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 134.53it/s, loss=0.00506] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.72321428571429\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 128.51it/s, loss=0.00156] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.74107142857143\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:02<00:00, 132.33it/s, loss=0.00123] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.75\n",
      "-> Saving CheckPoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:03<00:00, 110.76it/s, loss=0.00757] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 99.73214285714286\n",
      "-> Saving CheckPoint\n"
     ]
    }
   ],
   "source": [
    "# now training \n",
    "for epoch in range(num_epochs):\n",
    "    train(train_loader, model, optimizer, loss_fn, scaler)\n",
    "    \n",
    "    # save checkpoints \n",
    "    checkpoint = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer':optimizer.state_dict()\n",
    "    }\n",
    "    save_checkpoint(checkpoint)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T15:15:08.539870Z",
     "iopub.status.busy": "2023-01-18T15:15:08.538780Z",
     "iopub.status.idle": "2023-01-18T15:15:08.548579Z",
     "shell.execute_reply": "2023-01-18T15:15:08.547244Z",
     "shell.execute_reply.started": "2023-01-18T15:15:08.539798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 4078]) torch.Size([4078, 300])\n",
      "torch.Size([300, 4078])\n"
     ]
    }
   ],
   "source": [
    "w1 = model.state_dict()['model.0.weight']\n",
    "w2 = model.state_dict()['model.2.weight']\n",
    "print(w1.shape, w2.shape)\n",
    "weights = w1 + w2.T\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Now we have to simply assign the embeddings to their words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T15:20:48.330931Z",
     "iopub.status.busy": "2023-01-18T15:20:48.329884Z",
     "iopub.status.idle": "2023-01-18T15:20:48.336268Z",
     "shell.execute_reply": "2023-01-18T15:20:48.335022Z",
     "shell.execute_reply.started": "2023-01-18T15:20:48.330881Z"
    }
   },
   "outputs": [],
   "source": [
    "def embeddings(weights, ind2word = ind2word):\n",
    "    weights = weights.detach().cpu().numpy()\n",
    "    embed = {}\n",
    "    for key in ind2word.keys():\n",
    "        embed[ind2word[key]] = weights[:, key]\n",
    "    return embed\n",
    "\n",
    "######################\n",
    "word_embeddings = embeddings(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TESTING THE QUALITY OF WORD EMBEDDINGS THROUGH VISUALIZATION AND ANALOGIES**\n",
    "- We can apply many things here, first of all we can simply check if the classes are predicted correct or not on a given example.\n",
    "- Then we can apply PCA to the embddings and plot them to visulaize correlations between the words and get a understanding of how correct the embddings are.\n",
    "- As embeddings are built for use for some task, the ultimate check of them is to apply to that particular task and then get feedback from there, like if i wished to design embeddings for question-answering then i will train model and then how good the model is doing in question-answering, this will ultimately tell us the quality of our embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
